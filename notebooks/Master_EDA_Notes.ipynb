{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f21ef6e",
   "metadata": {},
   "source": [
    "## Contains EDA Separately Analyzed from the notebook - eda_experiments.ipynb in the notebooks folder.\n",
    "\n",
    "\n",
    "#### Doc Owner - Joel Markapudi.\n",
    "#### Date - 05/10/2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c103c",
   "metadata": {},
   "source": [
    "\n",
    "### Starting Analysis\n",
    "\n",
    "\n",
    "### First View Analysis On Content:\n",
    "\n",
    "Three columns contain nested JSON/dict structures:\n",
    "\n",
    "```\n",
    "Column          Structure                               Content\n",
    "labels          {'1d': 0, '30d': 0, '5d': 1}        Binary classification targets (market movement?)\n",
    "returns         Nested dict with 1d/5d/30d keys     Stock price data: closePriceStartDate, closePriceEndDate, \n",
    "                                                    ret (return %), date\n",
    "tickers         [AIR]                               List of ticker symbols\n",
    "```\n",
    "\n",
    "\n",
    "1. These are ML targets. Cannot directly store these in Qdrant payload (need flattening)\n",
    "2. Tickers list: Understand this better, soon.\n",
    "\n",
    "```\n",
    "Row 0: \"ITEM 1.BUSINESS General AAR CORP. and its subsidiaries...\" (180 chars)\n",
    "Row 1: \"AAR was founded in 1951, organized in 1955...\" (84 chars)\n",
    "Row 2: \"We are a diversified provider of products...\" (121 chars)\n",
    "```\n",
    "\n",
    "1. Sentences are coherent, complete thoughts.\n",
    "\n",
    "```\n",
    "\"ITEM 1.BUSINESS General AAR CORP. and its subsidiaries are referred to...\"\n",
    " └──────────────┘\n",
    "   This part\n",
    "```\n",
    "- The text itself starts with \"ITEM 1.BUSINESS\" as part of the sentence content. It's not in a separate column - it's embedded in the sentence text.\n",
    "- Are cik, section, reportDate actually useful for filtering? (Do they have good distribution?)\n",
    "- Are there other patterns we haven't discovered yet?\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### From Next cells:\n",
    "- Expected: 10 sections (0-9 for 10-K items)\n",
    "- Actual: section has 20 unique values\n",
    "    - What this means:\n",
    "    - Either sections go beyond 0-9 (10-19 range?)\n",
    "    - OR there are sub-sections (e.g., 1.1, 1.2 encoded as 11, 12?) \n",
    "\n",
    "```\n",
    "Field\t    Unique Values\t    Interpretation\t                Filtering Value\n",
    "cik\t        10\t                10 companies only\t            ✅ HIGH - company filter\n",
    "name\t    10\t                1:1 with CIK\t                ✅ HIGH - display name\n",
    "tickers\t    10\t                1:1 with company\t            ✅ HIGH - user-friendly filter\n",
    "section\t    20\t                ⚠️ MORE than expected\t        ✅ HIGH - but needs mapping\n",
    "docID\t    188\t                ~19 docs/company\t            ✅ MEDIUM - document-level filter\n",
    "filingDate\t181\t                Nearly 1:1 with docID\t        ⚠️ LOW - use reportDate instead\n",
    "reportDate\t91\t                ~2 filings/period\t            ✅ MEDIUM - time filter\n",
    "sentence\t96,465\t            48% unique\t                    ❌ N/A - this is the content\n",
    "sentenceID\t200,000\t            100% unique\t                    ✅ HIGH - primary key\n",
    "```\n",
    "\n",
    "\n",
    "- 188 documents total across 10 companies:\n",
    "    - Average: 18.8 documents per company\n",
    "    - This suggests ~19 years of filings (if annual 10-Ks)\n",
    "- 91 unique reportDates vs 181 filingDates:\n",
    "    - Multiple companies share same fiscal year-ends\n",
    "    - Common fiscal year-ends: Dec 31, Jun 30, Sep 30\n",
    "    - filingDate spread (181 unique) = different filing times\n",
    "\n",
    "- Three binary sentiment labels (1d, 5d, 30d) derived from market reaction windows.\n",
    "- Potential supervised target for finetuning sentiment or volatility predictors.\n",
    "- Label-conditioned embeddings: correlate language tone with short-term market moves.\n",
    "- Later: contrastive training of “risk-positive vs risk-negative” sentences.\n",
    "#### Metadata Richness:\n",
    "- can enrich embeddings with these categorical tags (via adapters or metadata vectors).\n",
    "- these features are extremely useful for metadata-filtered search and bias analysis later.\n",
    "\n",
    "#### Text Content Insights\n",
    "\n",
    "- 96,465 unique sentences out of 200,000 total:\n",
    "    - Duplication rate: 51.7% (103,535 repeated sentences)\n",
    "    - This is NORMAL for 10-Ks because:\n",
    "        - Boilerplate language (risk disclaimers, accounting policies)\n",
    "        - Repeated across years (\"We are incorporated in Delaware...\")\n",
    "        - Standard regulatory phrases\n",
    "\n",
    "For RAG:\n",
    "- Duplicates are FINE (same sentence, different context/year)\n",
    "- Embeddings will cluster similar content\n",
    "\n",
    "\n",
    "#### Low-value fields (don't change, not useful for filtering):\n",
    "```\n",
    "Field                   Unique      Why Low Value\n",
    "entityType              1           Always \"operating\" (no variance)\n",
    "tickerCount             1           Always 1 ticker (derived field)\n",
    "form                    1           Always \"10-K\" (dataset definition)\n",
    "exchanges               3           Only 3 exchanges (NYSE, NASDAQ, ?) - low filtering value\n",
    "stateOfIncorporation    5           Only 5 states (mostly DE) - not relevant for financial analysis\n",
    "```\n",
    "\n",
    "Medium-value field:\n",
    "```\n",
    "Field                   Unique      Potential Uses\n",
    "sic10                   10          Industry codes (1:1 with company, but useful for \"show me tech companies\")\n",
    "```\n",
    "\n",
    "#### ML Target Fields (Finance-Specific)\n",
    "- labels structure: {'1d': 0, '30d': 0, '5d': 1}\n",
    "- Binary classification: Did stock go UP (1) or DOWN/FLAT (0)?\n",
    "- Timeframes: 1-day, 5-day, 30-day post-filing\n",
    "- 8 unique combinations = all possible 0/1 patterns (2³ = 8)\n",
    "- returns structure: Nested price data\n",
    "- 188 unique values = 1 per document (document-level targets, not sentence-level)\n",
    "- Contains: start/end prices, return %, dates\n",
    "- Used for regression tasks (predict magnitude of movement)\n",
    "\n",
    "\n",
    "#### Temporal Metadata Analysis\n",
    "Three timestamp fields:\n",
    "```\n",
    "Field               Unique        Granularity        Use Case\n",
    "reportDate          91            Fiscal period end  ✅ \"Show me Q4 2019 results\"\n",
    "filingDate          181           SEC filing date    ⚠️ \"When did market learn this?\"\n",
    "acceptanceDateTime  188           Exact timestamp    ❌ Too granular (hour/minute irrelevant)\n",
    "```\n",
    "- Why reportDate (91) < filingDate (181)?\n",
    "- Multiple companies file on same calendar date (e.g., 2020-02-28)\n",
    "- But their fiscal year-ends differ (some Dec 31, some Jan 31, some custom)\n",
    "- ❌ labels, returns (ML targets)\n",
    "- ❌ entityType, tickerCount, form (no variance)\n",
    "- ❌ exchanges, stateOfIncorporation (low value)\n",
    "- ❌ acceptanceDateTime (too granular)\n",
    "- ❌ sentenceCount (internal counter)\n",
    "\n",
    "#### Chunking:\n",
    "- Chunking window ≈ 3–5 sentences will yield ~100–150 tokens — perfect for encoder models (MiniLM, E5, Titan Embeddings).\n",
    "- Avoid over-chunking: 10-K prose is repetitive; smaller chunks improve recall for factual retrieval.\n",
    "- “sentenceID” includes hierarchy: cik_form_year_section_index — perfect for traceability and citation.\n",
    "\n",
    "| Consideration        | Insight from EDA                                                | Recommended Approach                                                         |\n",
    "| -------------------- | --------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n",
    "| **Chunk length**     | Sentences average 28 words; coherent across 2–5-sentence spans. | Use sliding window of 3 sentences (≈ 100–150 tokens).                        |\n",
    "| **Chunk boundaries** | Section IDs (0–19) define strong topical boundaries.            | Chunk within each section; reset window at new section.                      |\n",
    "| **Metadata filters** | CIK, section, reportDate are perfectly populated.               | Use these as metadata filters in vector DB (OpenSearch).                     |\n",
    "| **Embedding schema** | Text + metadata + docID                                         | Each vector record → `{cik, section, reportDate, sentence_text, embedding}`. |\n",
    "| **Edge cases**       | Section imbalance (see dataset card: Item 7 > Item 14 etc.)     | Weighted sampling or per-section retrieval balancing.                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0910f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef249be6",
   "metadata": {},
   "source": [
    "### Part 2, 3 : Distribution Analysis & Sections. And, Text Density Analysis. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95b156",
   "metadata": {},
   "source": [
    "### EDA Deep Analysis - Part 1: Company, Temporal & Section Analysis\n",
    "\n",
    "### 1. COMPANY DISTRIBUTION (Q2) - SEVERE IMBALANCE\n",
    "\n",
    "**Table 3 Summary:**\n",
    "\n",
    "| Company | Ticker | SIC | Sentences | Filings | Date Range | % of Total |\n",
    "|---------|--------|-----|-----------|---------|------------|------------|\n",
    "| ADVANCED MICRO DEVICES INC | N/A | 3674 | 38,799 | 24 | 1993-2020 | 19.4% |\n",
    "| ABBOTT LABORATORIES | N/A | 2834 | 30,554 | 25 | 1993-2020 | 15.3% |\n",
    "| Air Products & Chemicals | N/A | 2810 | 26,282 | 20 | 2001-2020 | 13.1% |\n",
    "| CECO ENVIRONMENTAL CORP | N/A | 3564 | 24,867 | 17 | 2004-2020 | 12.4% |\n",
    "| AAR CORP | N/A | 3720 | 20,350 | 21 | 1994-2020 | 10.2% |\n",
    "| BK Technologies Corp | N/A | 3663 | 19,081 | 21 | 1995-2020 | 9.5% |\n",
    "| ACME UNITED CORP | N/A | 3420 | 15,849 | 26 | 1995-2020 | 7.9% |\n",
    "| ADAMS RESOURCES | N/A | 5172 | 14,964 | 19 | 2002-2020 | 7.5% |\n",
    "| WORLDS INC | N/A | 7372 | 7,797 | 13 | 2008-2020 | 3.9% |\n",
    "| Matson, Inc. | N/A | 4400 | 1,457 | 2 | 2019-2020 | 0.7% |\n",
    "\n",
    "**Key Stats:**\n",
    "- **Imbalance ratio: 26.63x** (AMD: 38,799 vs Matson: 1,457)\n",
    "- Std deviation: 10,874 sentences\n",
    "- Industry diversity: semiconductors, pharma, chemicals, aerospace, energy, shipping\n",
    "\n",
    "**Impact on RAG:**\n",
    "- Retrieval bias toward AMD (20x more chunks than Matson)\n",
    "- Matson effectively invisible without weighting\n",
    "- **Solution needed:** Per-company retrieval quotas OR stratified sampling\n",
    "\n",
    "---\n",
    "\n",
    "### 2. TEMPORAL DISTRIBUTION (Q3) - RECENCY BIAS\n",
    "\n",
    "**Coverage Timeline:**\n",
    "\n",
    "| Period | Sentences/Year | Filings/Year | Phase |\n",
    "|--------|----------------|--------------|-------|\n",
    "| 1993-2001 | 370-1,510 | 1-3 | Sparse (5% of data) |\n",
    "| 2002 (inflection) | 6,361 | 7 | Major jump |\n",
    "| 2002-2020 | 9,000-12,600 | 8-10 | Stable (95% of data) |\n",
    "\n",
    "**Key Findings:**\n",
    "- Total span: 28 years (1993-2020)\n",
    "- Usable data: 18 years (2002-2020 only)\n",
    "- **1999 anomaly:** Only 370 sentences (data gap)\n",
    "- **2020 peak:** 12,595 sentences (COVID disclosures)\n",
    "- **NOT evenly spread** - 95% concentration post-2002\n",
    "\n",
    "**Recommendation:** Filter `reportDate >= \"2002-01-01\"` for reliable temporal analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 3. SECTION CODES (Q1) - 20 SECTIONS DECODED\n",
    "\n",
    "**Major Sections:**\n",
    "\n",
    "| Section | 10-K Item | Sentences | % | Avg Tokens | Status |\n",
    "|---------|-----------|-----------|---|------------|--------|\n",
    "| **10** | **Notes to Financials** | **60,256** | **30.1%** | 26.2 | **CRITICAL** |\n",
    "| **8** | MD&A | 47,677 | 23.8% | 26.0 | High value |\n",
    "| 1 | Risk Factors | 24,627 | 12.3% | 27.7 | High value |\n",
    "| 0 | Business | 21,311 | 10.7% | 25.4 | High value |\n",
    "| **19** | Exhibits | 14,312 | 7.2% | 28.4 | Boilerplate |\n",
    "| 4 | Legal Proceedings | 4,534 | 2.3% | 23.6 | Standard |\n",
    "| 9 | Financial Statements | 3,993 | 2.0% | 22.7 | Tables |\n",
    "| 5 | Mine Safety | 3,893 | 1.9% | 11.6 | Standard |\n",
    "| 6 | Market for Stock | 2,836 | 1.4% | 23.5 | Standard |\n",
    "| 3 | Properties | 2,317 | 1.2% | 20.7 | Standard |\n",
    "| **2** | Unresolved Comments | 374 | 0.2% | **4.5** | **NOISE** |\n",
    "| **7** | Reserved | 1,355 | 0.7% | 20.5 | **SPARSE** |\n",
    "| **11** | Market Risk | 608 | 0.3% | **10.3** | **NOISE** |\n",
    "| **13** | Unknown | 479 | 0.2% | **4.7** | **NOISE** |\n",
    "\n",
    "**Extended Sections (11-19):** Controls, certifications, exhibits - mostly < 1% each\n",
    "\n",
    "**Critical Insights:**\n",
    "- **Section 10 (Notes) is THE priority** for KPI context (30% of all data)\n",
    "- Sections 0, 1, 8, 10 = **85% of data** (focus here for RAG)\n",
    "- Sections 2, 7, 11, 13, 17 = **NOISE** (< 1%, fragment sentences like \"See Exhibit 10.1\")\n",
    "- Section 19 (Exhibits) = 7% but legal lists (low semantic value)\n",
    "\n",
    "**Section Code Mapping:**\n",
    "```\n",
    "CORE (0-9):\n",
    "0  → Item 1: Business\n",
    "1  → Item 1A: Risk Factors\n",
    "2  → Item 1B: Unresolved Staff Comments (SPARSE)\n",
    "3  → Item 2: Properties\n",
    "4  → Item 3: Legal Proceedings\n",
    "5  → Item 4: Mine Safety\n",
    "6  → Item 5: Market for Stock\n",
    "7  → Item 6: Reserved (EMPTY)\n",
    "8  → Item 7: MD&A\n",
    "9  → Item 8: Financial Statements\n",
    "\n",
    "EXTENDED (10-19):\n",
    "10 → Notes to Financial Statements (DOMINANT)\n",
    "11 → Quantitative Market Risk\n",
    "12 → Controls & Procedures\n",
    "13 → Unknown (SPARSE)\n",
    "14 → Principal Accountant Fees\n",
    "15 → Exhibits Index\n",
    "16 → Form 10-K Summary\n",
    "17 → Unknown (SPARSE)\n",
    "18 → Unknown\n",
    "19 → Exhibit Documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ANSWERS TO 3 OPEN QUESTIONS\n",
    "\n",
    "**Q1: What are the 20 section codes?**  \n",
    "Sections 0-9 = standard 10-K items. **Section 10 = Notes to Financial Statements (30% of data - THE KEY SECTION for KPI context).** Sections 11-19 = extended disclosures (exhibits, certifications) - mostly noise.\n",
    "\n",
    "**Q2: Are companies evenly distributed?**  \n",
    "**NO.** Severe imbalance: 26.63x ratio (AMD 19.4% vs Matson 0.7%). Must implement per-company retrieval quotas or stratified sampling to prevent AMD bias.\n",
    "\n",
    "**Q3: Are filings evenly spread over time?**  \n",
    "**NO.** Heavy recency bias: 95% of data is post-2002. Pre-2002 period (1993-2001) is sparse and unreliable for trend analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8147a73",
   "metadata": {},
   "source": [
    "\n",
    "### 4. TEXT DENSITY & CHUNK SIZE VALIDATION\n",
    "\n",
    "### Overall Token Statistics\n",
    "\n",
    "| Metric | Value | Implication |\n",
    "|--------|-------|-------------|\n",
    "| Mean | 25.8 tokens/sentence | Typical sentence length |\n",
    "| Median | 22 tokens | Normal distribution (not skewed) |\n",
    "| P95 | 55 tokens | Outliers start beyond this |\n",
    "| Max | 737 tokens | Tables-as-text (toxic) |\n",
    "\n",
    "### Chunk Size Validation\n",
    "\n",
    "```\n",
    "3-sentence chunks → ~77 tokens   ✅ SAFE (well under 512 limit)\n",
    "5-sentence chunks → ~129 tokens  ✅ SAFE (comfortable margin)\n",
    "19 sentences max  → ~512 tokens  ⚠️ Theoretical max (not recommended)\n",
    "```\n",
    "\n",
    "**Recommendation:** 3-sentence sliding window with 1-sentence stride\n",
    "- Average: 77 tokens/chunk\n",
    "- Overlap: 2 sentences preserved (context continuity)\n",
    "- Output: ~200k chunks from 200k sentences\n",
    "- Rationale: Prevents topic drift (financial text jumps topics frequently)\n",
    "\n",
    "---\n",
    "\n",
    "### Section-Specific Density Analysis\n",
    "\n",
    "**Table 6: Text Density by Section**\n",
    "\n",
    "| Section | Item | Avg Tokens | Median | Max | Quality Rating |\n",
    "|---------|------|------------|--------|-----|----------------|\n",
    "| **12** | Controls | **33.4** | 28 | 179 | Densest (regulatory) |\n",
    "| **19** | Exhibits | 28.4 | 22 | 672 | Dense but boilerplate |\n",
    "| **1** | Risks | 27.7 | 24 | 362 | Good semantic content |\n",
    "| 10 | Notes | 26.2 | 23 | 428 | **IDEAL for RAG** |\n",
    "| 8 | MD&A | 26.0 | 23 | 433 | **IDEAL for RAG** |\n",
    "| 0 | Business | 25.4 | 21 | 737 | Good semantic content |\n",
    "| 5 | Mine Safety | **11.6** | 10 | 101 | Sparse |\n",
    "| **11** | Market Risk | **10.3** | 2 | 113 | **SPARSE/NOISE** |\n",
    "| **2** | Unresolved | **4.5** | 2 | 55 | **FRAGMENT SENTENCES** |\n",
    "| **13** | Unknown | **4.7** | 3 | 74 | **FRAGMENT SENTENCES** |\n",
    "\n",
    "**Sections Good for RAG:** 0, 1, 8, 10 (25-28 tokens, coherent narratives)  \n",
    "**Sections Bad for RAG:** 2, 7, 11, 13 (4-10 tokens, fragments like \"San Francisco, CA\")\n",
    "\n",
    "---\n",
    "\n",
    "### 5. OUTLIER ANALYSIS - DATA QUALITY FLAGS\n",
    "\n",
    "### Extreme Outliers (>1000 chars)\n",
    "\n",
    "| Section | Chars | Content Type | Example Preview |\n",
    "|---------|-------|--------------|-----------------|\n",
    "| 1 (Risks) | 1,174 | Legal disclaimers | \"Consequently, we are subject to military conflicts, civil...\" |\n",
    "| 10 (Notes) | 1,022 | **Financial table as text** | \"Sales by segment for these customers are as follows: AAR CORP...\" |\n",
    "| 12 (Controls) | 1,040 | Regulatory boilerplate | \"The Company's internal control over financial reporting is a process...\" |\n",
    "| 19 (Exhibits) | 1,330 | **Exhibit list** | \"4.3 Description of Capital Stock (filed herewith) 4.4 Rights Agreement...\" |\n",
    "| 19 (Exhibits) | 1,850 | **Material contracts list** | \"Material Contracts 10.1* Amended and Restated AAR CORP. Stock Benefit...\" |\n",
    "\n",
    "**Problem:** These break embeddings (737-token max observed → truncation) and have no semantic value\n",
    "\n",
    "**Solution:** Filter sentences > 500 chars (keeps P95+ data, removes 2% toxic outliers)\n",
    "\n",
    "---\n",
    "\n",
    "### ACTIONABLE DECISIONS FOR RAG PIPELINE\n",
    "\n",
    "### Decision 1: Filtering Strategy\n",
    "\n",
    "```python\n",
    "df_clean = df.filter(\n",
    "    # Remove noise sections\n",
    "    ~pl.col(\"section\").is_in([2, 7, 11, 13, 17]) &\n",
    "    \n",
    "    # Remove outliers (tables-as-text, exhibit lists)\n",
    "    (pl.col(\"sentence\").str.len_chars() <= 500) &\n",
    "    \n",
    "    # Remove sparse temporal data\n",
    "    (pl.col(\"reportDate\") >= \"2002-01-01\")\n",
    ")\n",
    "\n",
    "# Expected result: ~180k sentences (removes 10% noise, keeps 90% quality data)\n",
    "```\n",
    "\n",
    "**Impact:**\n",
    "- Removes 2, 7, 11, 13, 17 (< 2.5% of data, fragments)\n",
    "- Removes outliers > 500 chars (~2% of data, tables/lists)\n",
    "- Removes pre-2002 data (~5% of data, sparse coverage)\n",
    "- **Total removed: ~10% | Quality retained: ~90%**\n",
    "\n",
    "---\n",
    "\n",
    "### Decision 2: Chunking Strategy\n",
    "\n",
    "**Recommended: 3-sentence sliding window, 1-sentence stride**\n",
    "\n",
    "**Rationale:**\n",
    "- 77 tokens avg (safe for 512-token models)\n",
    "- Preserves context via overlap\n",
    "- Prevents topic drift (financial text jumps topics frequently: KPI → explanation → next KPI)\n",
    "- Shorter chunks = better precision for KPI extraction\n",
    "\n",
    "**Alternative considered:** 5-sentence chunks (129 tokens)\n",
    "- Rejected because: longer chunks risk topic drift within chunk\n",
    "\n",
    "---\n",
    "\n",
    "### Decision 3: Company Balancing\n",
    "\n",
    "**Problem:** 26.63x imbalance means AMD dominates retrieval\n",
    "\n",
    "**Options:**\n",
    "\n",
    "| Strategy | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **A. Downsample AMD/Abbott** | Balanced training | Loses information |\n",
    "| **B. Weighted retrieval** | Keeps all data | Complex implementation |\n",
    "| **C. Per-company quotas** | Guarantees diversity | May miss best match |\n",
    "\n",
    "**Recommendation: Option C** (Per-company retrieval quotas)\n",
    "- Retrieve top-3 results per company\n",
    "- Then rank all 30 results by similarity\n",
    "- **Why:** FinSight KPI extraction benefits from company diversity (prevents \"AMD-only\" responses)\n",
    "\n",
    "---\n",
    "\n",
    "### Decision 4: Priority Sections for RAG\n",
    "\n",
    "**Focus on these sections (85% of data):**\n",
    "\n",
    "1. **Section 10 (30%)** - Notes to Financial Statements → KPI context, explanations\n",
    "2. **Section 8 (24%)** - MD&A → Narrative analysis, trends\n",
    "3. **Section 1 (12%)** - Risk Factors → Qualitative insights\n",
    "4. **Section 0 (11%)** - Business → Company overview, revenue streams\n",
    "\n",
    "**Optionally include:**\n",
    "- Section 4 (2.3%) - Legal Proceedings (if relevant)\n",
    "- Section 9 (2.0%) - Financial Statements (tables - handle carefully)\n",
    "\n",
    "**Exclude:**\n",
    "- Sections 2, 7, 11, 13, 17 (noise)\n",
    "- Section 19 (7%) - Exhibits (boilerplate lists)\n",
    "\n",
    "---\n",
    "\n",
    "### METADATA FOR QDRANT PAYLOAD - potential schema\n",
    "\n",
    "**Based on analysis, recommended payload schema:**\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"chunk_id\": \"0000001750_10-K_2020_section_8_chunk_42\",\n",
    "    \"text\": \"[3-sentence chunk text]\",\n",
    "    \"cik\": \"0000001750\",\n",
    "    \"company\": \"AAR CORP\",\n",
    "    \"ticker\": \"AIR\",\n",
    "    \"section\": 8,                  # Section code (0-19)\n",
    "    \"reportDate\": \"2020-05-31\",\n",
    "    \"docID\": \"0000001750_10-K_2020\",\n",
    "    \"sic\": \"3720\"                   # Industry code (optional)\n",
    "}\n",
    "```\n",
    "\n",
    "**Filterable fields:** `cik`, `section`, `reportDate`, `ticker`  \n",
    "**Stored but not indexed:** `docID`, `sic`, `company`\n",
    "\n",
    "---\n",
    "\n",
    "### SUMMARY: KEY TAKEAWAYS\n",
    "\n",
    "### Data Characteristics\n",
    "- 200k sentences → **~180k usable** (after filtering)\n",
    "- 10 companies, **severe imbalance** (26.63x)\n",
    "- 28-year span, **95% post-2002** (recency bias)\n",
    "- 20 sections, **4 sections = 85% of data** (0, 1, 8, 10)\n",
    "\n",
    "### Text Properties\n",
    "- Average: 25.8 tokens/sentence\n",
    "- 3-sentence chunks: 77 tokens (safe for embeddings)\n",
    "- Outliers: 2% of data (tables-as-text, exhibit lists)\n",
    "\n",
    "### Critical Sections\n",
    "- **Section 10 (30%)**: THE priority for KPI context\n",
    "- Sections 0, 1, 8: High-value narrative content\n",
    "- Sections 2, 7, 11, 13: Noise (filter out)\n",
    "\n",
    "### Required Actions\n",
    "1. Filter noise sections (2, 7, 11, 13, 17)\n",
    "2. Remove outliers (> 500 chars)\n",
    "3. Use 3-sentence sliding window chunking\n",
    "4. Implement per-company retrieval quotas\n",
    "5. Filter reportDate >= 2002-01-01\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to Section 1.4 (if needed): N-gram analysis, vocabulary patterns ?? Think about this. This is small_full."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbac8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85802b53",
   "metadata": {},
   "source": [
    "\n",
    "### Deep-dive insights from EDA (Q2 → end)\n",
    "\n",
    "### 1) Company distribution (Q2)\n",
    "\n",
    "**What you found:** 10 companies, **200,000 sentences** total; **large imbalance** (e.g., AMD ≈ 38.8k sentences vs Matson ≈ 1.5k). **Imbalance ratio ~26.6×**; filings per company vary (2 → 26) and span **1993–2020**.\n",
    "\n",
    "**Why this matters**\n",
    "\n",
    "* **Index skew**: a few firms dominate the vector index. Pure ANN retrieval may bias toward overrepresented writing styles/phrases.\n",
    "* **Evaluation skew**: if your gold set concentrates in “big” companies/years, you’ll overestimate performance.\n",
    "\n",
    "**Actions**\n",
    "\n",
    "* **Balanced gold set**: sample 2–3 filings per company across early/mid/late years (e.g., 2000, 2010, 2019) → fair coverage.\n",
    "* **Index caps**: per company, cap max vectors per section/year (or down-weight when ranking).\n",
    "* **Stratified eval**: report metrics per-company and macro-average across companies so small issuers don’t get hidden.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Temporal distribution by year (Q3)\n",
    "\n",
    "**What you found:** coverage **1993–2020**, steady growth post-2002 (SOX era) and again in late 2010s. Sentences/year ~7.1k on avg, peaks around 2020.\n",
    "\n",
    "**Why this matters**\n",
    "\n",
    "* **Language drift**: disclosure tone, accounting phrasing, and risk taxonomy evolved.\n",
    "* **Section composition shift**: some items (e.g., MD&A, controls, exhibits) grew over time.\n",
    "\n",
    "**Actions**\n",
    "\n",
    "* **Decade shards**: (optional) build decade/tag filters to study retrieval drift (90s/00s/10s).\n",
    "* **Recency weighting**: for live use, prefer latest year passages when period isn’t explicit.\n",
    "* **Generalization check**: train prompts/heuristics on pre-2015 filings, validate on 2016–2020; watch drops.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Section code distribution (Q1) & cross-company heatmap\n",
    "\n",
    "**What you found:** **20 section codes (0–19)**, not just 0–9. Heavy hitters: **10 (30.1%)**, **8 (23.8%)**, **1 (12.3%)**, **0 (10.7%)**, **19 (7.2%)**. Very light: **2, 11, 13, 15, 17**. Heatmap confirms most companies populate the heavy sections; some sparsity in others.\n",
    "\n",
    "**Interpretation (practical)**\n",
    "\n",
    "* The dataset collapses more than the canonical 10 items—likely sub-items/appendices are mapped to higher codes (10–19).\n",
    "* High-volume sections (8/10/1/0) drive most of your retrieval hits; thin sections will hurt recall if you rely on them.\n",
    "\n",
    "**Retrieval priors (policy)**\n",
    "\n",
    "* **KPI extraction** → bias to **8, 10** (financial statements & notes) and **7/MD&A-like** areas if present.\n",
    "* **Risk/Drivers** → bias to **1A/7-like** codes (your heavy **1, 0** buckets often carry business/MD&A-type prose).\n",
    "* Keep a **down-weight** for **19** (exhibits/references) unless you specifically need exhibits.\n",
    "\n",
    "*(Later, we can learn a compact mapping “code → canonical item label” by sampling top n-grams per code.)*\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Token length distribution & chunking\n",
    "\n",
    "**What you found:** **Mean ~25.8 tokens/sentence**, **p95 ~55**, **max ~737** (tables/lists).\n",
    "Your table of density by section shows **avg tokens** vary widely (**section 12 ~33.4** densest; **section 2 ~4.5** sparsest).\n",
    "\n",
    "**Decisions**\n",
    "\n",
    "* **Adaptive chunking** (per section density):\n",
    "\n",
    "  * **Dense sections (avg ≥ ~26 tokens)** → **3-sentence window**, **1-sentence overlap**.\n",
    "  * **Medium (18–26)** → **4-sentence window**, **1-sentence overlap**.\n",
    "  * **Sparse (≤ ~18)** → **5–6 sentences**, **2-sentence overlap**.\n",
    "* **Hard caps**: truncate chunks at **~150–200 tokens** (keeps encoders efficient; nice fit for rerankers too).\n",
    "* **Reset on section change** to avoid cross-topic chunks.\n",
    "\n",
    "**Outlier handling**\n",
    "\n",
    "* Sentences **>1000 chars** are often lists/tables/exhibits; treat as **table-like**.\n",
    "\n",
    "  * If KPI-targeted, run a **regex/table parser** path; else **exclude from text embeddings** to reduce noise.\n",
    "  * Tag these in metadata (`is_table_like=1`) for optional specialized handling.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Duplicates & boilerplate\n",
    "\n",
    "**What you observed in your notes:** ~**52%** of sentences are duplicates (not surprising): boilerplate risk/legal text, multi-year carry-overs.\n",
    "\n",
    "**Why this helps (if managed)**\n",
    "\n",
    "* Embeddings of duplicated boilerplate will cluster; ANN can over-return them.\n",
    "\n",
    "**Actions**\n",
    "\n",
    "* **Near-duplicate suppression at index time**: within each `(cik, section, decade)` drop vectors with cosine sim ≥ **0.97** to an existing exemplar.\n",
    "* **Query-time down-weight** duplicates (feature `dup_count`) so unique, data-rich passages rank higher.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) Metadata you can reliably filter on\n",
    "\n",
    "* **High value**: `cik`, `reportDate`, `section`, `docID` (audit trail), `sentenceID` (citation), `sic` (industry).\n",
    "* **Low value**: `entityType`, `tickerCount`, `form` (constant); `stateOfIncorporation`, `exchanges` (coarse, rarely useful).\n",
    "* **Targets**: `labels`, `returns` at **document** level (not sentence level) — useful for downstream supervised tasks, not for RAG retrieval directly.\n",
    "\n",
    "**OpenSearch mapping sketch**\n",
    "\n",
    "* `text`: the chunk text\n",
    "* `vector`: dense embedding\n",
    "* `cik` (keyword), `reportDate` (date), `section` (short), `docID` (keyword), `sentence_span` (short), `sic` (keyword)\n",
    "* `char_len`, `token_len`, `is_table_like`, `dup_count` (ints) for ranking rules\n",
    "\n",
    "---\n",
    "\n",
    "### 7) Retrieval strategy that fits these distributions\n",
    "\n",
    "1. **Constrain early with metadata**: `(cik, reportDate)` (if user specified), then **section priors** by intent.\n",
    "2. **Hybrid search**: vector ANN + **keyword filters** (“in millions”, “Net sales”, KPI labels) improves precision in dense sections.\n",
    "3. **Rerank small k** (optional later): a cross-encoder reranker (or Bedrock “judge” prompt) on top-30 → top-5 improves faithfulness.\n",
    "4. **Evidence guardrails**: only accept KPI if **evidence sentence contains the number & scale tokens** (prevents “off-by-scale” errors).\n",
    "\n",
    "---\n",
    "\n",
    "### 8) KPI extraction implications\n",
    "\n",
    "* Most KPI sentences will live in **8/10**; narrative drivers in **1/0/7-like**.\n",
    "* Your **unit normalization** must handle “in millions/billions” headers; add a **page/paragraph-level scope detector** (regex on a few neighboring chunks).\n",
    "* **Period alignment**: prefer `reportDate` for fiscal tagging; if period text is ambiguous in MD&A, fall back to the **nearest financial-statement chunk** for the same metric.\n",
    "\n",
    "---\n",
    "\n",
    "### 9) Evaluation slices to add (so results are credible)\n",
    "\n",
    "Report all metrics **by**:\n",
    "\n",
    "* **Company size**: top-3 vs bottom-3 by sentence_count\n",
    "* **Year bucket**: pre-2005, 2005–2014, 2015–2020\n",
    "* **Section group**: {8/10}, {1/0/7}, {others}\n",
    "\n",
    "This guards against a system that looks good only on AMD-style heavy disclosures or only on recent years.\n",
    "\n",
    "---\n",
    "\n",
    "### 10) Concrete next steps (fast to execute)\n",
    "\n",
    "1. **Build a section-aware chunker** with the adaptive window rules above (store `token_len`, `is_table_like`).\n",
    "2. **Index with duplicate suppression** (cos ≥ 0.97 within `(cik, section, decade)`).\n",
    "3. **Write retrieval policies** (KPI vs Narrative) with section priors and a few keyword hints per KPI (e.g., Revenue, Net income, R&D, Operating income).\n",
    "4. **Assemble a balanced gold set** (2–3 filings × 10 companies, spread across years) and lock the schema for scoring.\n",
    "5. **Run a small ablation**:\n",
    "\n",
    "   * fixed 3-sent chunks vs adaptive chunking\n",
    "   * vector-only vs hybrid+keyword filters\n",
    "   * with vs without duplicate suppression\n",
    "     → Pick the combo that maximizes Recall@10 and KPI EM on the gold set.\n",
    "\n",
    "---\n",
    "\n",
    "### TL;DR design decisions you can lock now\n",
    "\n",
    "* **Adaptive chunking** by section density; **reset at section boundaries**; cap at ~200 tokens; 1–2 sentence overlap.\n",
    "* **Index controls**: metadata filters; duplicate suppression; flag `is_table_like`.\n",
    "* **Retrieval priors**: KPI → {8,10}; Narrative → {1,0,7}; down-weight {19} unless needed.\n",
    "* **Evaluation**: stratify by company/period/section group; balance the gold set.\n",
    "* **Normalization**: enforce evidence-contains-number rule; handle “in millions/billions” via neighborhood regex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a780e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d9bf3bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69b31bf1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f9b533",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f20354b2",
   "metadata": {},
   "source": [
    "# Deep Analysis Addendum (Essential Highlights Only)\n",
    "\n",
    "This addendum captures **new, actionable** findings from the auxiliary analysis that complement our main EDA brief. It excludes items already covered or proven incorrect.\n",
    "\n",
    "---\n",
    "## A) KPI Signal Density — Where Numbers Actually Live\n",
    "\n",
    "**Why this matters:** Directly informs **section prioritization** for structured KPI extraction (numbers, units, EPS, YoY) vs. narrative-only RAG.\n",
    "\n",
    "**Key takeaways (consistent with our earlier EDA, now reinforced):**\n",
    "- **Item 8 (MD&A)** — highest overall KPI signal (currency %, growth verbs, some YoY): prime target for *numbers-with-explanations*.\n",
    "- **Item 10 (Financial Statements/Notes)** — strongest **EPS** & **units (“in millions/billions”)** signal: prime target for *audited KPI lines*.\n",
    "- **Item 7 (“Selected Financial Data” legacy / financials summary)** — surprisingly high **currency** and **units** despite being smaller: treat as **secondary KPI** source.\n",
    "- **Item 1 (Risk Factors)** — **narrative-rich** (growth verbs) but **number-poor**: keep for *explanations*, not for extraction.\n",
    "\n",
    "**Practical policy (KPI first-pass):**\n",
    "- **Extract** from: **8 (MD&A)** → **10 (Notes)** → **7 (Selected/Financials)**.\n",
    "- **Explain** from: **1 (Risks)** → **0 (Business)**.\n",
    "\n",
    "> This validates our **“KPI first, Narrative second”** routing and helps tune budgets (more LLM parsing time where numeric density is high).\n",
    "\n",
    "\n",
    "## B) Section Mapping (Cleaned)\n",
    "\n",
    "Use the **n-gram signatures + manual verification** to finalize a **human label** per section (only the parts that differed or sharpened our mapping):\n",
    "\n",
    "| Section | Human Label (for UI + Routing) | Notes (why) |\n",
    "|---:|---|---|\n",
    "| 0 | **Business / Overview** | Terms: products, sales, company, operations |\n",
    "| 1 | **Risk Factors** | Modal verbs (“may”, “could”), “risks” |\n",
    "| 2 | **Unresolved Staff Comments** | “item 1b”, “unresolved staff”, “comments none” → **boilerplate** |\n",
    "| 7 | **Selected Financial Data** (legacy) | “selected financial”, currency/units spikes |\n",
    "| 8 | **MD&A** | “million”, “sales”, “tax”, “income”, “cash” |\n",
    "| 9 | **Financial Statements** | Statements body (narrative around line items) |\n",
    "| 10 | **Notes to Financial Statements** | “financial”, “consolidated”, “december”, “value”, “stock” |\n",
    "| 11 | **Acct. Disagreements** | “disagreements with accountants”, typically **none** |\n",
    "| 12 | **Controls & Procedures** | “internal control”, “over financial reporting” |\n",
    "| 19 | **Exhibits & References** | “form”, “filed”, “report”, index-like cues |\n",
    "\n",
    "**Policy impact:**\n",
    "- Treat **2, 11** as **boilerplate / low-value** for KPI; keep searchable for compliance queries.\n",
    "- Bias KPI retrieval toward **8, 10, 7**; bias explanatory retrieval toward **1, 0**.\n",
    "\n",
    "\n",
    "\n",
    "## C) “Noise” Sections to Down-weight or Filter (KPI Path)\n",
    "\n",
    "Based on KPI-zero signals and boilerplate cues, **down-weight** (or **skip** for structured extraction) the following:\n",
    "\n",
    "- **2 – Unresolved Staff Comments** (compliance boilerplate)\n",
    "- **5 – (As surfaced: low/zero KPI signal in sample)**  \n",
    "- **11 – Disagreements with Accountants** (typically “none”)\n",
    "- **13 – (As surfaced: negligible KPI content in sample)**\n",
    "\n",
    "> Keep these **searchable** for niche questions, but do **not** spend LLM KPI budget here.\n",
    "\n",
    "\n",
    "## D) Section-Aware Chunk Size Defaults (Sharper)\n",
    "\n",
    "Use KPI density to guide default chunk size:\n",
    "\n",
    "- **KPI-dense (7, 8, 10):** **2–3 sentences** (precise spans; avoid diluting with narrative)  \n",
    "- **Narrative-heavy (0, 1):** **4–5 sentences** (context matters; still cap ~200 tokens)  \n",
    "\n",
    "Always **reset at section boundaries** and **flag table-like outliers** (long lists/tables) for specialized handling.\n",
    "\n",
    "\n",
    "## E) Query Routing Patterns (Refined Cheatsheet)\n",
    "\n",
    "Minimal, high-signal routing based on section labels and n-gram cues:\n",
    "\n",
    "- **KPI intents** → boost **8, 10, 7**  \n",
    "  Regex hints: `revenue|net income|operating income|EPS|gross margin|R&D|cash|capex|tax`\n",
    "- **Risk/Qualitative intents** → boost **1, 0**  \n",
    "  Hints: `risk|threat|challenge|uncertainty|supply chain|macro`\n",
    "- **Controls/Compliance** → boost **12, 11, 19**  \n",
    "  Hints: `internal control|disclosure|procedure|exhibit|agreement`\n",
    "\n",
    "> Combine **metadata filters** (`cik`, `reportDate`) with **section boosts** for first-pass retrieval.\n",
    "\n",
    "\n",
    "\n",
    "## F) Important Correction — Duplication Estimation\n",
    "\n",
    "**Do not** rely on the reported duplication rates where `n_near_dupes` >> `n_sampled` (impossible).  \n",
    "Likely issues: over-counting cluster pairs, bucket collisions, or cross-section contamination.\n",
    "\n",
    "**What to keep:**  \n",
    "- The **directional** reminder that **Risk Factors** and **Controls** carry more boilerplate;  \n",
    "- The **principle** to **apply near-duplicate suppression** (cosine or SimHash) **within** `(docID, section, decade)`.\n",
    "\n",
    "**What to fix later:**  \n",
    "- Recompute with **unique cluster counting** (e.g., LSH clusters → count `cluster_size - 1` once),  \n",
    "- Or run **cosine-based suppression** on embeddings directly during index build.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26618e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Deep-Dive EDA Briefing (for SEC 10-K sentence dataset)\n",
    "\n",
    "### 1) What each artifact tells us (and why it matters)\n",
    "\n",
    "**A. `top_ngrams_by_section.csv` — Section “language fingerprint”**\n",
    "\n",
    "* You computed TF-IDF top n-grams per section, sampled per section, (1,2)-grams with sensible DF thresholds. This gives a *signature vocabulary* for each section (e.g., Item 1: “business”, “segment”, “customers”; Item 1A: “risk”, “adverse”; Item 7: “management discussion”, “operations”, etc.). \n",
    "* **Why it matters**:\n",
    "\n",
    "  * Improves retrieval by adding a prior: given a user intent (e.g., “risks of supply chain”), boost sections whose n-grams match the intent.\n",
    "  * Enables **section-aware chunking** and **router prompts** (see §3).\n",
    "\n",
    "**B. `section_label_suggestions.csv` — Human-readable section mapping**\n",
    "\n",
    "* You mapped those n-gram signatures to readable labels (Business/Overview, Risk Factors, MD&A, Financial Statements/Notes, Controls & Procedures, Legal/Exhibits). This is exactly the bridge from opaque numeric `section` codes (0–19) to practical filters in UX and routing. \n",
    "* **Why it matters**:\n",
    "\n",
    "  * Gives you a clean **taxonomy** to anchor UI filters, metadata filters in retrieval, and evaluation slices.\n",
    "\n",
    "**C. `duplication_by_section.csv` — Near-duplicate pressure by section**\n",
    "\n",
    "* Using a SimHash-style approach (shingles→64-bit hash→prefix buckets→sampled pair checks), you estimated near-duplication rates per section. Given the overall dataset has ~48% unique sentences (duplication is normal in 10-Ks), this tells you where boilerplate repeats the most.  \n",
    "* **Why it matters**:\n",
    "\n",
    "  * Guides **index compaction** (dedupe or down-weight duplicates inside the ANN index).\n",
    "  * Helps **evidence diversity**: when forming a context window, avoid stuffing multiple near-duplicates—use one with strongest metadata match.\n",
    "\n",
    "**D. `kpi_signal_scan_by_section.csv` — Where the numbers live**\n",
    "\n",
    "* Regex probes for currency, percent, EPS, units (thousands/millions/billions), YoY/growth verbs, by section with per-section sampling. It tells you *where structured KPIs are likely extractable* (e.g., high numeric density in Financial Statements/Notes and MD&A; lower in Legal/Exhibits). \n",
    "* **Why it matters**:\n",
    "\n",
    "  * Narrows **extractor scope** (prioritize sections with high numeric signal).\n",
    "  * Drives **prompt specialization** (use a KPI template only when signal ≥ threshold).\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Cross-checks from your notebook (foundation facts)\n",
    "\n",
    "* **Scale/shape**: 200,000 rows × 19 cols; ~144 MB in memory for the small_full parquet in Polars. \n",
    "* **Time span**: Coverage ~1993–2020 (28 years). Useful for period filters & drift checks. \n",
    "* **Section codes**: 20 unique (0–19), not just 0–9. Some are exhibits/controls; mapping via label suggestions is needed for UX and routing. \n",
    "* **Token lengths**: Mean ≈26 tokens/sentence; p95 ≈55; long tails often tables/lists (outliers >1000 chars) in items like 10, 12, 19. Chunking should treat **table-like spans** differently (capture intact or skip). \n",
    "* **Company imbalance**: Sentence volume per company is imbalanced (expected with 28-year span). Retrieval should **favor doc/time filters** to avoid over-representing prolific issuers. \n",
    "\n",
    "---\n",
    "\n",
    "### 3) Design decisions this EDA unlocks (actionable)\n",
    "\n",
    "**3.1 Chunking & indexing**\n",
    "\n",
    "* **Unit of chunk**: start with **3–5 sentences** (≈ 75–130 tokens avg), which sits well under most 512-token embedding limits and captures local context for KPI lines plus the immediate explanation. Your token stats support this.\n",
    "* **Table-like outliers**: Detect via `char_count > 1000` (your rule); either (a) capture as **verbatim block** in a separate “table” index with table-aware embedding, or (b) **skip** them for narrative retrieval and rely on structured extraction sourced from those sections when needed. \n",
    "* **Metadata keys** (store with each vector): `cik`, `name`, `docID`, `section`, `reportDate`, `year`, and your **human section label** from `section_label_suggestions.csv`. These power facet filters and UX pivots. \n",
    "\n",
    "**3.2 Retrieval & routing**\n",
    "\n",
    "* **Intent → Section boost**: Map user intents (e.g., “risk” / “MD&A outlook” / “revenue growth”) to section labels using `top_ngrams_by_section.csv` signatures. Apply a **pre-filter or boost** at retrieval time (metadata filter + query rewrite with section terms). \n",
    "* **De-dup policy**:\n",
    "\n",
    "  * **Index-time**: if `dup_rate` is high for a section, keep only one vector per near-duplicate cluster per docID (or store all but mark duplicates with a lower weight).\n",
    "  * **Query-time**: apply a **diversity constraint**: no two contexts with Hamming distance ≤ T (or same sentence hash) in the final top-k. \n",
    "* **Temporal filter**: Default to `reportDate` window for comparability (e.g., “show last 3 years”), with optional `filingDate` when event-time matters (market reaction labels are keyed to filing). \n",
    "\n",
    "**3.3 KPI extractor scope**\n",
    "\n",
    "* Use `kpi_signal_scan_by_section.csv` to prioritize **Financial Statements/Notes** and **MD&A** for number extraction. Trigger the KPI extractor only if a chunk (or its neighbors) trips **numeric cues** (currency/percent/units/EPS/YoY). \n",
    "* For **auditability**, always store: `(value, unit, KPI_name guess, period anchor, section, docID, exact sentence span)` and return the **evidence sentenceID** with the answer.\n",
    "\n",
    "**3.4 Prompting patterns**\n",
    "\n",
    "* **Retrieval prompt**: seed with section label hints (e.g., “Prefer Item 7 (MD&A) when user asks about management’s analysis…”).\n",
    "* **KPI prompt**: strict JSON schema with fields for `value`, `unit`, `period`, `as_of_date`, `evidence_sentenceID`; include a *refusal rule* if no explicit numeric evidence is present.\n",
    "* **Narrative prompt**: cite `[sentenceID]` after each claim; instruct model to avoid deriving numbers—only restate or contextualize.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) What to do with each CSV (practical use)\n",
    "\n",
    "* **`section_label_suggestions.csv`** → load into a small mapping table for your pipeline; expose in the UI as human-friendly filters; use in query routing and eval slicing. \n",
    "* **`top_ngrams_by_section.csv`** → build a simple **intent→section** lookup: when a query contains “risk”, “adverse”, “uncertainty”, boost Risk Factors; for “revenue”, “gross margin”, boost MD&A/FS&Notes. This can be a dictionary + cosine over n-gram expansions. \n",
    "* **`duplication_by_section.csv`** → set **per-section** dedupe thresholds (e.g., stricter in Items with boilerplate); also report **coverage after dedupe** to ensure recall isn’t harmed. \n",
    "* **`kpi_signal_scan_by_section.csv`** → configure **extractor budgets** (LLM calls/time) where signal is high; in low-signal sections, skip extractor and rely on narrative search only. \n",
    "\n",
    "---\n",
    "\n",
    "### 5) Section-wise expectations (policy you can codify)\n",
    "\n",
    "* **Item 1 (Business/Overview)**: narrative heavy; useful for qualitative Q&A; numeric signal moderate (market/segment sizes appear occasionally).\n",
    "* **Item 1A (Risk Factors)**: little structured KPI; high duplication potential across years (boilerplate); emphasize diversity + recency.\n",
    "* **Item 7 (MD&A)**: rich numeric context (growth %, YoY, driver explanations); **top priority** for KPI+explanations pairing.\n",
    "* **Item 8/Notes (Financial Statements & Notes)**: dense with currency/units; great for **audited KPIs**; tables often exceed normal chunk size → handle with table mode.\n",
    "* **Controls/Exhibits**: low KPI value; keep for compliance/explanations but deprioritize for extraction.\n",
    "\n",
    "(These align with your outlier check and the n-gram label suggestions.)  \n",
    "\n",
    "---\n",
    "\n",
    "### 6) Evaluation slices you can build from here\n",
    "\n",
    "* **By section label**: retrieval recall@k and answer correctness for MD&A vs Risk vs FS/Notes.\n",
    "* **By period**: pre/post 2008, or rolling 5-year windows, to detect drift.\n",
    "* **With/without dedupe**: show impact on recall and answer diversity.\n",
    "* **KPI hit rate**: fraction of user KPI intents that produce a validated number+evidence (use your `kpi_signal_scan` to define eligible queries).\n",
    "\n",
    "---\n",
    "\n",
    "### 7) Immediate next steps (short list)\n",
    "\n",
    "1. **Lock the section taxonomy**: freeze the mapping from `section`→label using your `section_label_suggestions.csv` (review a few sections manually). \n",
    "2. **Implement section-aware retrieval**: add label boosts guided by `top_ngrams_by_section.csv`. \n",
    "3. **Add dedupe at retrieval time**: diversity constraint over near-dupe pairs per `duplication_by_section.csv`. \n",
    "4. **Gate the KPI extractor**: only run when numeric cues are detected (and in high-signal sections per `kpi_signal_scan_by_section.csv`). \n",
    "5. **Table handling**: send table-like chunks to a separate path (either skip for narrative RAG or process with a table parser). \n",
    "\n",
    "---\n",
    "\n",
    "### Citations to your notebook cells (provenance)\n",
    "\n",
    "* N-gram signature creation & save (`top_ngrams_by_section.csv`)  \n",
    "* Duplication estimation & save (`duplication_by_section.csv`)  \n",
    "* KPI signal scan & save (`kpi_signal_scan_by_section.csv`)  \n",
    "* Section label suggestions & save (`section_label_suggestions.csv`)  \n",
    "* Dataset size and memory footprint (Polars printout) \n",
    "* Temporal coverage table & stats \n",
    "* Outlier (table-like) examples by section \n",
    "* Section code distribution (0–19) and the need for mapping \n",
    "* Company distribution/imbalance table & summary \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab466510",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369b80f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0491fc20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbc69e91",
   "metadata": {},
   "source": [
    "## REMEMBER THIS:\n",
    "- ❌ Deep EDA on small_full before testing large_full\n",
    "- Your section distributions WILL change\n",
    "- Company balance WILL change\n",
    "- Token stats might shift (if large_full has different companies/years)\n",
    "- ❌ Assuming small_full is representative\n",
    "- It's called \"small\" for a reason\n",
    "- Likely a curated subset (e.g., only 10 companies, only 2002-2020)\n",
    "- Large_full might include 100+ companies, 1990-2023, international filings\n",
    "- ❌ Perfectionism on the wrong dataset\n",
    "\n",
    "- Even if absolute numbers (counts, medians, token lengths) shift later, the qualitative shape of the data — structure, hierarchies, field types, sparsity, edge cases — rarely changes between the small and large splits.\n",
    "    - The schema (cik, section, sentence, returns) is fixed.\n",
    "    - The distribution form (e.g., some sections heavy, some sparse; certain companies dominating) will stay the same.\n",
    "    - Anomalies like duplicates, boilerplate, table-like text are systemic, not random — they appear everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1fac9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrag_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
