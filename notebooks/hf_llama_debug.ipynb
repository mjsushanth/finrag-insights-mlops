{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75181c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e63d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HF CACHES, DATASET DELETES:\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "\n",
    "# These are unrelated to your KPI extraction\n",
    "to_remove = [\n",
    "    \n",
    "    \"datasets--JanosAudran--financial-reports-sec\",  # 13.41 GB\n",
    "    \"models--openai--clip-vit-large-patch14\",  # 1.71 GB \n",
    "    \"models--CompVis--stable-diffusion-safety-checker\",  # 1.22 GB - SD\n",
    "    \"datasets--TeoGchx--HumanML3D\",  # Motion capture dataset\n",
    "]\n",
    "\n",
    "for model in to_remove:\n",
    "    path = cache_dir / model\n",
    "    if path.exists():\n",
    "        print(f\"Removing {model}...\")\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"  Freed {model.split('--')[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ccf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HF CACHE ANALYSIS:\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_dir_size(path: Path) -> int:\n",
    "    \"\"\"Return total size (in bytes) of all files under given path.\"\"\"\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            try:\n",
    "                fp = Path(root) / f\n",
    "                total += fp.stat().st_size\n",
    "            except (FileNotFoundError, PermissionError):\n",
    "                pass\n",
    "    return total\n",
    "\n",
    "def human_readable(size_bytes: int) -> str:\n",
    "    \"\"\"Format bytes into human-readable units.\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "\n",
    "# Hugging Face cache locations (adjust if needed)\n",
    "hf_base = Path.home() / \".cache\" / \"huggingface\"\n",
    "paths = {\n",
    "    \"models\": hf_base / \"hub\" / \"models\",\n",
    "    \"datasets\": hf_base / \"datasets\",\n",
    "    \"transformers\": hf_base / \"transformers\",\n",
    "    \"diffusers\": hf_base / \"diffusers\",\n",
    "    \"others\": hf_base / \"hub\",\n",
    "}\n",
    "\n",
    "print(f\"\\n=== Hugging Face Cache Analysis ===\\nBase path: {hf_base}\\n\")\n",
    "\n",
    "if not hf_base.exists():\n",
    "    print(\"No Hugging Face cache found.\")\n",
    "else:\n",
    "    total = 0\n",
    "    for name, path in paths.items():\n",
    "        if path.exists():\n",
    "            size = get_dir_size(path)\n",
    "            total += size\n",
    "            print(f\"{name:<15}: {human_readable(size)} ({path})\")\n",
    "    print(f\"\\nTotal estimated cache size: {human_readable(total)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342abc4",
   "metadata": {},
   "source": [
    "# Minimal Ollama model manager with a VS Code/Jupyter-friendly progress bar\n",
    "# pip install requests tqdm\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Any, List\n",
    "from tqdm.auto import tqdm  # <- works in VS Code notebooks and Jupyter\n",
    "\n",
    "class OllamaManager:\n",
    "    def __init__(self, host: str = \"http://localhost:11434\", timeout: int = 120):\n",
    "        self.base = host.rstrip(\"/\")\n",
    "        self.timeout = timeout\n",
    "\n",
    "    # --- Internal helpers ---\n",
    "    def _post(self, path: str, json_body: Dict[str, Any], stream: bool = False):\n",
    "        url = f\"{self.base}{path}\"\n",
    "        r = requests.post(url, json=json_body, timeout=self.timeout, stream=stream)\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "\n",
    "    def _get(self, path: str):\n",
    "        url = f\"{self.base}{path}\"\n",
    "        r = requests.get(url, timeout=self.timeout)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    # --- API ---\n",
    "    def list_models(self) -> List[Dict[str, Any]]:\n",
    "        return self._get(\"/api/tags\").get(\"models\", [])\n",
    "\n",
    "    def delete(self, model: str) -> Dict[str, Any]:\n",
    "        return self._post(\"/api/delete\", {\"name\": model}).json()\n",
    "\n",
    "    def show(self, model: str) -> Dict[str, Any]:\n",
    "        return self._post(\"/api/show\", {\"name\": model}).json()\n",
    "\n",
    "    def server_ok(self) -> bool:\n",
    "        try:\n",
    "            self.list_models()\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    # --- Pull with progress ---\n",
    "    def pull(self, model: str, insecure: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Download a model and show a live progress bar in VS Code/Jupyter.\n",
    "        Example: mgr.pull(\"qwen2.5:14b-instruct-q4_K_M\")\n",
    "        \"\"\"\n",
    "        payload = {\"name\": model, \"insecure\": insecure}\n",
    "        resp = self._post(\"/api/pull\", payload, stream=True)\n",
    "\n",
    "        pbar = None\n",
    "        last = {}\n",
    "        current_desc = \"downloading\"\n",
    "\n",
    "        # chunk_size=1 to flush lines quickly; decode text\n",
    "        for line in resp.iter_lines(chunk_size=1, decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Each line is a JSON object from Ollama\n",
    "            msg = json.loads(line)\n",
    "            last = msg\n",
    "            status = msg.get(\"status\") or current_desc\n",
    "            completed = int(msg.get(\"completed\", 0) or 0)\n",
    "            total = int(msg.get(\"total\", 0) or 0)\n",
    "\n",
    "            if total > 0:\n",
    "                if pbar is None:\n",
    "                    pbar = tqdm(total=total, unit=\"B\", unit_scale=True,\n",
    "                                desc=status, dynamic_ncols=True, leave=False)\n",
    "                # If total changes across layers, update bar total\n",
    "                if total != pbar.total:\n",
    "                    pbar.total = total\n",
    "                pbar.n = completed\n",
    "                if status != current_desc:\n",
    "                    pbar.set_description(status)\n",
    "                    current_desc = status\n",
    "                pbar.refresh()\n",
    "            else:\n",
    "                # Manifest/layer messages without totals\n",
    "                print(status, flush=True)\n",
    "\n",
    "        if pbar:\n",
    "            pbar.close()\n",
    "        print(f\"✅ Download completed: {model}\")\n",
    "        return last\n",
    "\n",
    "\n",
    "\n",
    "mgr = OllamaManager()\n",
    "if not mgr.server_ok():\n",
    "    print(\"Start Ollama first (run the Ollama app/daemon).\")\n",
    "\n",
    "mgr.pull(\"qwen2.5:14b-instruct-q4_K_M\")\n",
    "\n",
    "mgr.pull(\"qwen2.5:7b-instruct-q4_K_M\")\n",
    "\n",
    "## check models.\n",
    "print([m[\"name\"] for m in mgr.list_models()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\"\n",
    "\n",
    "def analyze_cache():\n",
    "    models = {}\n",
    "    \n",
    "    # Check hub folder\n",
    "    hub_dir = cache_dir / \"hub\"\n",
    "    if hub_dir.exists():\n",
    "        for model_dir in hub_dir.iterdir():\n",
    "            if model_dir.is_dir():\n",
    "                size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())\n",
    "                models[model_dir.name] = size / 1e9  # GB\n",
    "    \n",
    "    # Check transformers folder (old cache)\n",
    "    transformers_dir = cache_dir / \"transformers\"\n",
    "    if transformers_dir.exists():\n",
    "        size = sum(f.stat().st_size for f in transformers_dir.rglob('*') if f.is_file())\n",
    "        models['transformers_legacy'] = size / 1e9\n",
    "    \n",
    "    # Sort by size\n",
    "    sorted_models = sorted(models.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Cache Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, size in sorted_models[:20]:  # Top 20\n",
    "        print(f\"{size:6.2f} GB  {name}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total: {sum(models.values()):.2f} GB\")\n",
    "    \n",
    "    # Suggest cleanup\n",
    "    if sum(models.values()) > 10:\n",
    "        print(\"\\nTo clean specific models:\")\n",
    "        print(\"shutil.rmtree(cache_dir / 'hub' / 'models--MODEL_NAME')\")\n",
    "\n",
    "analyze_cache()\n",
    "\n",
    "# \"\"\"\n",
    "# Cache Analysis:\n",
    "# --------------------------------------------------\n",
    "#  13.41 GB  datasets--JanosAudran--financial-reports-sec\n",
    "#   1.71 GB  models--openai--clip-vit-large-patch14\n",
    "#   1.22 GB  models--CompVis--stable-diffusion-safety-checker\n",
    "#   0.50 GB  models--deepset--roberta-base-squad2\n",
    "#   0.44 GB  models--sentence-transformers--all-mpnet-base-v2\n",
    "#   0.26 GB  models--distilbert-base-cased-distilled-squad\n",
    "#   0.23 GB  models--mrm8488--bert-small-finetuned-squadv2\n",
    "#   0.00 GB  models--bert-base-uncased\n",
    "#   0.00 GB  models--distilbert-base-uncased\n",
    "#   0.00 GB  datasets--TeoGchx--HumanML3D\n",
    "#   0.00 GB  .locks\n",
    "# --------------------------------------------------\n",
    "# Total: 17.76 GB\n",
    "\n",
    "# To clean specific models:\n",
    "# shutil.rmtree(cache_dir / 'hub' / 'models--MODEL_NAME')\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28262cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56d32e14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0651fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: {\"metrics\":[{\"metric\":\"revenue\",\"value\":2500,\"unit\":\"millions\"}]}\n"
     ]
    }
   ],
   "source": [
    "## MODEL DEBUGS 1:\n",
    "\n",
    "# Debug extraction - see raw Ollama output\n",
    "def debug_extract(sentence, model=\"qwen2.5:14b-instruct-q4_K_M\"):\n",
    "    import requests\n",
    "    \n",
    "    # Simpler prompt\n",
    "    prompt = f\"\"\"Extract financial numbers from: {sentence}\n",
    "Return JSON array like: [{{\"metric\":\"revenue\",\"value\":100,\"unit\":\"millions\"}}]\n",
    "If no metrics, return: []\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": \"json\",\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    resp = requests.post(\"http://localhost:11434/api/generate\", json=payload, timeout=30)\n",
    "    \n",
    "    if resp.status_code == 200:\n",
    "        raw = resp.json()\n",
    "        print(f\"Raw response: {raw.get('response', 'NO RESPONSE')}\")\n",
    "        return raw\n",
    "    else:\n",
    "        print(f\"Error: {resp.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Test on a known financial sentence\n",
    "test_sentence = \"The company reported revenue of $2.5 billion in fiscal 2023.\"\n",
    "result = debug_extract(test_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411eee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944079e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b1c68c4",
   "metadata": {},
   "source": [
    "## Llama server cpp style + model download; Using Qwen2.5-3B-Instruct-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11cdc27a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (425907411.py, line 148)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 148\u001b[39m\n\u001b[31m    \u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "def download_model(url: str, dst_path: str, chunk_size: int = 1<<20):\n",
    "    \"\"\"Download model with progress bar and resume support\"\"\"\n",
    "    \n",
    "    dst_path = Path(dst_path)\n",
    "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if file already exists and is complete\n",
    "    if dst_path.exists():\n",
    "        existing_size = dst_path.stat().st_size\n",
    "        \n",
    "        # Get expected file size from server\n",
    "        head_response = requests.head(url, allow_redirects=True)\n",
    "        expected_size = int(head_response.headers.get('content-length', 0))\n",
    "        \n",
    "        if existing_size == expected_size and expected_size > 0:\n",
    "            print(f\"✓ File already exists and is complete: {dst_path.name}\")\n",
    "            print(f\"  Size: {existing_size/1e9:.2f} GB\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"⚠ Incomplete file found ({existing_size/1e6:.1f}MB), re-downloading...\")\n",
    "            dst_path.unlink()\n",
    "    \n",
    "    print(f\"↓ Downloading: {dst_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=30) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            downloaded = 0\n",
    "            \n",
    "            with open(dst_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        \n",
    "                        if total_size > 0:\n",
    "                            pct = (downloaded / total_size) * 100\n",
    "                            gb_done = downloaded / 1e9\n",
    "                            gb_total = total_size / 1e9\n",
    "                            \n",
    "                            print(f\"\\r  Progress: {pct:6.2f}% ({gb_done:.2f}GB/{gb_total:.2f}GB)\", end=\"\", flush=True)\n",
    "        \n",
    "        print(f\"\\n✓ Downloaded: {dst_path.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Download failed: {e}\")\n",
    "        if dst_path.exists():\n",
    "            dst_path.unlink()\n",
    "        raise\n",
    "\n",
    "def merge_split_files(part_files: List[Path], output_path: Path):\n",
    "    \"\"\"Merge split GGUF files into single file\"\"\"\n",
    "    print(f\"\\n→ Merging {len(part_files)} parts into {output_path.name}\")\n",
    "    \n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'wb') as outfile:\n",
    "        for i, part_file in enumerate(sorted(part_files), 1):\n",
    "            print(f\"  Merging part {i}/{len(part_files)}: {part_file.name}\")\n",
    "            with open(part_file, 'rb') as infile:\n",
    "                outfile.write(infile.read())\n",
    "    \n",
    "    print(f\"✓ Merged successfully: {output_path}\")\n",
    "    print(f\"  Final size: {output_path.stat().st_size/1e9:.2f} GB\")\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"qwen2.5-3b\": {\n",
    "        \"url\": \"https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf\",\n",
    "        \"path\": r\"C:\\llama_server\\models\\qwen2p5_3b\\Qwen2.5-3B-Instruct-Q4_K_M.gguf\",\n",
    "        \"size\": \"~2.2GB\",\n",
    "        \"split\": False\n",
    "    },\n",
    "    \"qwen2.5-7b\": {\n",
    "        # Q4_K_M is split into 2 parts\n",
    "        \"urls\": [\n",
    "            \"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf\",\n",
    "            \"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf\"\n",
    "        ],\n",
    "        \"temp_dir\": r\"C:\\llama_server\\models\\qwen2p5_7b\\parts\",\n",
    "        \"path\": r\"C:\\llama_server\\models\\qwen2p5_7b\\Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n",
    "        \"size\": \"~4.7GB\",\n",
    "        \"split\": True\n",
    "    },\n",
    "    \"qwen2.5-7b-bartowski\": {\n",
    "        # Bartowski version - single file, easier!\n",
    "        \"url\": \"https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n",
    "        \"path\": r\"C:\\llama_server\\models\\qwen2p5_7b\\Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n",
    "        \"size\": \"~4.7GB\",\n",
    "        \"split\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "def download_qwen_model(model_key: str):\n",
    "    \"\"\"Download a Qwen model (handles split files automatically)\"\"\"\n",
    "    \n",
    "    model_config = MODELS[model_key]\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"Downloading: {model_key}\")\n",
    "    print(f\"Expected size: {model_config['size']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if model_config[\"split\"]:\n",
    "        # Download split files\n",
    "        temp_dir = Path(model_config[\"temp_dir\"])\n",
    "        temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        part_files = []\n",
    "        for i, url in enumerate(model_config[\"urls\"], 1):\n",
    "            filename = url.split(\"/\")[-1]\n",
    "            part_path = temp_dir / filename\n",
    "            \n",
    "            print(f\"\\nPart {i}/{len(model_config['urls'])}: {filename}\")\n",
    "            download_model(url, str(part_path))\n",
    "            part_files.append(part_path)\n",
    "        \n",
    "        # Merge files\n",
    "        output_path = Path(model_config[\"path\"])\n",
    "        \n",
    "        if not output_path.exists():\n",
    "            merge_split_files(part_files, output_path)\n",
    "            \n",
    "            # Clean up parts\n",
    "            print(\"\\n→ Cleaning up part files...\")\n",
    "            for part_file in part_files:\n",
    "                part_file.unlink()\n",
    "                print(f\"  Deleted: {part_file.name}\")\n",
    "        else:\n",
    "            print(f\"\\n✓ Merged file already exists: {output_path}\")\n",
    "    else:\n",
    "        # Single file download\n",
    "        download_model(model_config[\"url\"], model_config[\"path\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Download complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # RECOMMENDED: Use bartowski's version - single file, no merge needed\n",
    "    # download_qwen_model(\"qwen2.5-7b-bartowski\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a71b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Qwen2.5-7B-Instruct Q5_K_M Model Downloader\n",
      "======================================================================\n",
      "\n",
      "Connecting to: https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q5_K_M.gguf\n",
      "\n",
      "Downloading: Qwen2.5-7B-Instruct-Q5_K_M.gguf\n",
      "Total size: 5.44 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|##########| 5.07G/5.07G [10:31<00:00, 8.62MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Download complete!\n",
      "  File: C:\\llama_server\\models\\qwen2p5_7b\\Qwen2.5-7B-Instruct-Q5_K_M.gguf\n",
      "  Size: 5.44 GB\n",
      "\n",
      "======================================================================\n",
      "Ready to use!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_model_efficient(url: str, dst_path: str, chunk_size: int = 8*1024*1024):\n",
    "    \"\"\"\n",
    "    Efficient model download with progress bar and resume support\n",
    "    Uses 8MB chunks for optimal download speed\n",
    "    \"\"\"\n",
    "    \n",
    "    dst_path = Path(dst_path)\n",
    "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if file already exists and get current size\n",
    "    resume_byte_pos = 0\n",
    "    if dst_path.exists():\n",
    "        resume_byte_pos = dst_path.stat().st_size\n",
    "        print(f\"Found existing file: {resume_byte_pos / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Get file info from server\n",
    "    print(f\"Connecting to: {url}\")\n",
    "    headers = {}\n",
    "    if resume_byte_pos > 0:\n",
    "        headers['Range'] = f'bytes={resume_byte_pos}-'\n",
    "    \n",
    "    response = requests.get(url, headers=headers, stream=True, timeout=30)\n",
    "    \n",
    "    # Get total file size\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    if resume_byte_pos > 0:\n",
    "        total_size += resume_byte_pos\n",
    "    \n",
    "    # Check if resume worked\n",
    "    if response.status_code == 206:\n",
    "        print(f\"Resuming download from {resume_byte_pos / 1e9:.2f} GB\")\n",
    "        mode = 'ab'\n",
    "    elif response.status_code == 200:\n",
    "        if resume_byte_pos > 0:\n",
    "            print(\"Server doesn't support resume, starting fresh\")\n",
    "            dst_path.unlink()\n",
    "            resume_byte_pos = 0\n",
    "        mode = 'wb'\n",
    "    else:\n",
    "        raise Exception(f\"Download failed with status code: {response.status_code}\")\n",
    "    \n",
    "    # Download with progress bar\n",
    "    print(f\"\\nDownloading: {dst_path.name}\")\n",
    "    print(f\"Total size: {total_size / 1e9:.2f} GB\")\n",
    "    print()\n",
    "    \n",
    "    with open(dst_path, mode) as f:\n",
    "        with tqdm(\n",
    "            total=total_size,\n",
    "            initial=resume_byte_pos,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "            desc='Progress',\n",
    "            ascii=True\n",
    "        ) as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    \n",
    "    # Verify download\n",
    "    final_size = dst_path.stat().st_size\n",
    "    print(f\"\\n✓ Download complete!\")\n",
    "    print(f\"  File: {dst_path}\")\n",
    "    print(f\"  Size: {final_size / 1e9:.2f} GB\")\n",
    "    \n",
    "    if total_size > 0 and final_size != total_size:\n",
    "        print(f\"⚠ Warning: File size mismatch (expected {total_size / 1e9:.2f} GB)\")\n",
    "    \n",
    "    return dst_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Model configuration\n",
    "    MODEL_URL = \"https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q5_K_M.gguf\"\n",
    "    MODEL_PATH = r\"C:\\llama_server\\models\\qwen2p5_7b\\Qwen2.5-7B-Instruct-Q5_K_M.gguf\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Qwen2.5-7B-Instruct Q5_K_M Model Downloader\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        download_model_efficient(MODEL_URL, MODEL_PATH)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Ready to use!\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠ Download interrupted\")\n",
    "        print(\"Run script again to resume from where you left off\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848da9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ef617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, os, sys\n",
    "\n",
    "LLAMA_DIR = r\"C:\\llama_server\\llama_cpp_b6814\"  # folder containing llama-server.exe\n",
    "MODEL     = r\"C:\\models\\qwen2p5_3b\\Qwen2.5-3B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "def start_llama_server(model_path=MODEL, port=8080, ctx=1024, parallel=16, gpu_layers=-1):\n",
    "    exe = os.path.join(LLAMA_DIR, \"llama-server.exe\")\n",
    "    args = [\n",
    "        exe, \"-m\", model_path, \"--api\",\n",
    "        \"--port\", str(port),\n",
    "        \"--ctx-size\", str(ctx),\n",
    "        \"--cont-batching\",\n",
    "        \"--parallel\", str(parallel),\n",
    "        \"--gpu-layers\", str(gpu_layers)\n",
    "    ]\n",
    "    print(\"Launching:\", \" \".join(args))\n",
    "    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    # Wait for it to bind the port and load the model\n",
    "    time.sleep(3)\n",
    "    # Print first few lines for sanity\n",
    "    for _ in range(10):\n",
    "        line = proc.stdout.readline()\n",
    "        if not line: break\n",
    "        print(line.rstrip())\n",
    "    return proc\n",
    "\n",
    "def stop_llama_server(proc):\n",
    "    if proc and proc.poll() is None:\n",
    "        proc.terminate()\n",
    "\n",
    "server_proc = start_llama_server()\n",
    "# When you're done later:\n",
    "# stop_llama_server(server_proc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8256988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-expressive but compact schema; lets the LLM invent KPI categories.\n",
    "PROMPT_TEMPLATE = \"\"\"You are a financial KPI extractor. Output ONLY a JSON array.\n",
    "Use dynamic categories (e.g., revenue, earnings, insurance_reserves, regulatory_capital,\n",
    "restructuring_metrics, employee_count, interest_rate_change, debt_issuance, etc.).\n",
    "Do not invent numbers. Omit null fields.\n",
    "\n",
    "Text: {text}\n",
    "Meta: sentenceID={sid}\n",
    "\n",
    "For each KPI in Text, include:\n",
    "- category: short snake_case category you choose\n",
    "- value: numeric (no commas); percentages as numbers (e.g., 92 means 92%)\n",
    "- unit: USD|USD_millions|USD_billions|percent|count|boe|rate|ratio|other\n",
    "- period_text: exact substring describing the period (if present)\n",
    "- year: YYYY if present\n",
    "- quarter: Q1|Q2|Q3|Q4 if present\n",
    "- explanation: <= 15 words (what/why)\n",
    "- spans: list of {{start,end}} char offsets for each numeric in Text\n",
    "- evidence_sentence_id: set to sentenceID\n",
    "\n",
    "If no KPIs, return [].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84be44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dbcc25c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrag_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
