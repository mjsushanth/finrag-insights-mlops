{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e7327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2b614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "349f447b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Tiers.\n",
    "1. Tier 1 - Definitely Process (likely ~15-20% of data): has_numbers=True AND (likely_kpi=True OR contains $,million,billion,%)\n",
    "2. Tier 2 - Skip Completely (likely ~70% of data): No numbers, no financial terms, pure legal text\n",
    "3. Tier 3 - Smart Sample (remaining ~10-15%): Has numbers but unclear context - sample 20% of these\n",
    "\n",
    "#### 1 clear failcase:\n",
    "1. Key Failures: Missing BOE reduction (85 million) and tax assets. model needs clearer prompting for non-monetary metrics.\n",
    "\n",
    "#### Prompt should guide extraction of:\n",
    "1. Value clusters - all numbers in context\n",
    "2. Temporal anchors - years, quarters, periods\n",
    "3. Causal relationships - \"due to\", \"driven by\", \"because of\"\n",
    "4. Comparative context - \"increased from\", \"compared to\"\n",
    "5. Operational context - what the number represents\n",
    "\n",
    "#### Checkpoint Strategy\n",
    "1. Save state every N sentences:\n",
    "  Last processed index, Extracted KPIs so far, Timestamp, Resume token ?\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"kpis\": [\n",
    "    {\n",
    "      \"category\": \"dynamic_from_llm\",  // Let LLM decide\n",
    "      \"subcategory\": \"optional\",        // More granular if needed\n",
    "      \"values\": [                        // Multiple values per KPI\n",
    "        {\"amount\": 750, \"unit\": \"USD_millions\"},\n",
    "        {\"amount\": 2.35, \"unit\": \"percent\"}\n",
    "      ],\n",
    "      \"period\": \"2017-05\",              // Standardized when possible\n",
    "      \"period_text\": \"May 2017\",        // Original text\n",
    "      \"comparison\": {                   // Optional\n",
    "        \"type\": \"YoY\",\n",
    "        \"prior_value\": 650,\n",
    "        \"change\": 15.4\n",
    "      },\n",
    "      \"explanation\": \"issued fixed-rate notes\",  // Context\n",
    "      \"confidence\": 0.85,\n",
    "      \"sentence_span\": [0, 150]         // Character positions\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"sentenceID\": \"xxx\",\n",
    "    \"extraction_timestamp\": \"2024-10-21T10:30:00Z\",\n",
    "    \"model_version\": \"qwen2.5-7b\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd2d51",
   "metadata": {},
   "source": [
    "### PRE FILTERING STEPS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4279ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 13:15:02,945 - Loading data with lazy evaluation...\n",
      "C:\\Users\\joems\\AppData\\Local\\Temp\\ipykernel_5568\\4112172597.py:24: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  total_count = df_lazy.select(pl.count()).collect().item()\n",
      "C:\\Users\\joems\\AppData\\Local\\Temp\\ipykernel_5568\\4112172597.py:25: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  filtered_count = filtered.select(pl.count()).collect().item()\n",
      "2025-10-21 13:15:03,249 - Original: 1,003,534 sentences\n",
      "2025-10-21 13:15:03,249 - After filtering: 564,551 sentences\n",
      "2025-10-21 13:15:03,249 - Reduction: 43.7%\n",
      "2025-10-21 13:15:03,249 - Processing time estimate: 2352.3 hours\n",
      "2025-10-21 13:15:03,249 - Collecting filtered data...\n",
      "2025-10-21 13:15:05,252 - Saved to: D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sec_finrag_1M_sample_filtered.parquet\n"
     ]
    }
   ],
   "source": [
    "# simple_prefilter.py\n",
    "import polars as pl\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def filter_for_kpi_extraction(data_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Simple 3-filter approach using lazy evaluation\"\"\"\n",
    "    \n",
    "    logger.info(\"Loading data with lazy evaluation...\")\n",
    "    \n",
    "    # Use scan for lazy evaluation - won't load into memory\n",
    "    df_lazy = pl.scan_parquet(data_path)\n",
    "    \n",
    "    # Apply just 3 filters\n",
    "    filtered = df_lazy.filter(\n",
    "        (pl.col(\"likely_kpi\") == True) | \n",
    "        (pl.col(\"has_numbers\") == True) | \n",
    "        (pl.col(\"sentence\").str.contains(r'\\d'))  # Any digit at all\n",
    "    )\n",
    "    \n",
    "    # Count without collecting full data\n",
    "    total_count = df_lazy.select(pl.count()).collect().item()\n",
    "    filtered_count = filtered.select(pl.count()).collect().item()\n",
    "    \n",
    "    logger.info(f\"Original: {total_count:,} sentences\")\n",
    "    logger.info(f\"After filtering: {filtered_count:,} sentences\")\n",
    "    logger.info(f\"Reduction: {((total_count - filtered_count) / total_count * 100):.1f}%\")\n",
    "    logger.info(f\"Processing time estimate: {(filtered_count * 15 / 3600):.1f} hours\")\n",
    "    \n",
    "    # Now collect the filtered data\n",
    "    logger.info(\"Collecting filtered data...\")\n",
    "    result_df = filtered.collect()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = r\"D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sec_finrag_1M_sample.parquet\"\n",
    "    \n",
    "    filtered_df = filter_for_kpi_extraction(data_path)\n",
    "    \n",
    "    # Save it\n",
    "    output_path = data_path.replace('.parquet', '_filtered.parquet')\n",
    "    filtered_df.write_parquet(output_path)\n",
    "    logger.info(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f129f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ALL COLUMNS IN DATASET:\n",
      "====================================================================================================\n",
      " 1. sample_id\n",
      " 2. cik\n",
      " 3. sentence\n",
      " 4. section\n",
      " 5. labels\n",
      " 6. filingDate\n",
      " 7. name\n",
      " 8. docID\n",
      " 9. sentenceID\n",
      "10. sentenceCount\n",
      "11. tickers\n",
      "12. exchanges\n",
      "13. entityType\n",
      "14. sic\n",
      "15. stateOfIncorporation\n",
      "16. tickerCount\n",
      "17. acceptanceDateTime\n",
      "18. form\n",
      "19. reportDate\n",
      "20. returns\n",
      "21. cik_int\n",
      "22. report_year\n",
      "23. temporal_bin\n",
      "24. sampling_rate_pct\n",
      "25. char_count\n",
      "26. word_count_approx\n",
      "27. sample_created_at\n",
      "28. last_modified_date\n",
      "29. sample_version\n",
      "30. source_file_path\n",
      "31. load_method\n",
      "32. record_status\n",
      "33. row_hash\n",
      "34. section_priority\n",
      "35. likely_kpi\n",
      "36. has_numbers\n",
      "37. is_table_like\n",
      "38. has_forward_looking\n",
      "39. has_comparison\n",
      "40. is_material\n",
      "41. mentions_years\n",
      "42. is_recent\n",
      "43. has_risk_language\n",
      "44. is_safe_harbor\n",
      "45. retrieval_signal_score\n",
      "\n",
      "====================================================================================================\n",
      "Total columns: 45\n",
      "Total rows: 564,551\n",
      "====================================================================================================\n",
      "\n",
      "FIRST ROW SAMPLE:\n",
      "====================================================================================================\n",
      "sample_id: 4\n",
      "cik: 0000109198\n",
      "sentence: Wright, is the middle to upper-middle income shopper, with the same profile as a department or speci...\n",
      "section: 0\n",
      "labels: {'1d': 0, '5d': 1, '30d': 1}\n",
      "filingDate: 2006-03-29\n",
      "name: TJX COMPANIES INC /DE/\n",
      "docID: 0000109198_10-K_2006\n",
      "sentenceID: 0000109198_10-K_2006_section_1_7\n",
      "sentenceCount: 41175086\n",
      "tickers: ['TJX']\n",
      "exchanges: ['NYSE']\n",
      "entityType: operating\n",
      "sic: 5651\n",
      "stateOfIncorporation: DE\n",
      "tickerCount: 1\n",
      "acceptanceDateTime: 2006-03-29T17:08:15.000Z\n",
      "form: 10-K\n",
      "reportDate: 2006-01-28\n",
      "returns: {'1d': {'closePriceEndDate': 5.036416530609131, 'closePriceStartDate': 5.000287055969238, 'endDate':...\n",
      "cik_int: 109198\n",
      "report_year: 2006\n",
      "temporal_bin: bin_2006_2009\n",
      "sampling_rate_pct: 28.59\n",
      "char_count: 120\n",
      "word_count_approx: 19\n",
      "sample_created_at: 2025-10-18 10:07:15.596070+00:00\n",
      "last_modified_date: 2025-10-18 10:07:15.596070+00:00\n",
      "sample_version: v1.0_75companies_1M\n",
      "source_file_path: D:/JoelDesktop folds_24/NEU FALL2025/MLops IE7374 Project/finrag-insights-mlops/data/exports/sec_fil...\n",
      "load_method: stratified_sampling\n",
      "record_status: complete\n",
      "row_hash: 14db83bb32ca6a6ff1b0a89812e13a3d\n",
      "section_priority: P2_HEADER\n",
      "likely_kpi: True\n",
      "has_numbers: False\n",
      "is_table_like: False\n",
      "has_forward_looking: False\n",
      "has_comparison: False\n",
      "is_material: False\n",
      "mentions_years: False\n",
      "is_recent: False\n",
      "has_risk_language: False\n",
      "is_safe_harbor: False\n",
      "retrieval_signal_score: None\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 1. Inspect Random Sample of Filtered Data. Inspect Columns. \n",
    "import polars as pl\n",
    "\n",
    "# Load the filtered data\n",
    "data_path = r\"D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sec_finrag_1M_sample_filtered.parquet\"\n",
    "\n",
    "df = pl.read_parquet(data_path)\n",
    "\n",
    "# Print ALL column names\n",
    "print(\"=\"*100)\n",
    "print(\"ALL COLUMNS IN DATASET:\")\n",
    "print(\"=\"*100)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Also print first row to see data structure\n",
    "print(\"\\nFIRST ROW SAMPLE:\")\n",
    "print(\"=\"*100)\n",
    "first_row = df.head(1).to_dicts()[0]\n",
    "for key, value in first_row.items():\n",
    "    # Truncate long values for display\n",
    "    val_str = str(value)\n",
    "    if len(val_str) > 100:\n",
    "        val_str = val_str[:100] + \"...\"\n",
    "    print(f\"{key}: {val_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebdfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Found 30 high-quality KPI sentences (both likely_kpi=True and has_numbers=True)\n",
      "========================================================================================================================\n",
      "\n",
      "✓ Saved 30 sentences to JSON\n",
      "  File: D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sample_30_kpi_sentences.json\n",
      "\n",
      "========================================================================================================================\n",
      "PREVIEW - First 3 sentences:\n",
      "========================================================================================================================\n",
      "[\n",
      "  {\n",
      "    \"sentence_id\": \"0000109198_10-K_2006_section_7_7\",\n",
      "    \"sentence_text\": \"RESULTS OF OPERATIONS Fiscal 2006 Overview: - Net sales for fiscal 2006 were $16.1 billion, an 8% increase over fiscal 2005.\",\n",
      "    \"metadata\": {\n",
      "      \"company_name\": \"TJX COMPANIES INC /DE/\",\n",
      "      \"cik\": \"0000109198\",\n",
      "      \"document_id\": \"0000109198_10-K_2006\",\n",
      "      \"section\": 8,\n",
      "      \"report_year\": 2006,\n",
      "      \"filing_date\": \"2006-03-29\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"sentence_id\": \"0000109198_10-K_2006_section_7_19\",\n",
      "    \"sentence_text\": \"- Fourth quarter results for fiscal 2006 were stronger than earlier quarters, with same store sales that increased 3% and pre-tax margins that grew from 6.1% last year to 7.5% this year.\",\n",
      "    \"metadata\": {\n",
      "      \"company_name\": \"TJX COMPANIES INC /DE/\",\n",
      "      \"cik\": \"0000109198\",\n",
      "      \"document_id\": \"0000109198_10-K_2006\",\n",
      "      \"section\": 8,\n",
      "      \"report_year\": 2006,\n",
      "      \"filing_date\": \"2006-03-29\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"sentence_id\": \"0000109198_10-K_2006_section_7_23\",\n",
      "    \"sentence_text\": \"During fiscal 2006, we repurchased 25.9 million of our shares at a cost of $600 million, which favorably affected our earnings per share.\",\n",
      "    \"metadata\": {\n",
      "      \"company_name\": \"TJX COMPANIES INC /DE/\",\n",
      "      \"cik\": \"0000109198\",\n",
      "      \"document_id\": \"0000109198_10-K_2006\",\n",
      "      \"section\": 8,\n",
      "      \"report_year\": 2006,\n",
      "      \"filing_date\": \"2006-03-29\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "========================================================================================================================\n",
      "JSON Structure:\n",
      "========================================================================================================================\n",
      "  - dataset: metadata about the collection\n",
      "  - filter_criteria: what filters were applied\n",
      "  - total_sentences: count\n",
      "  - sentences[]: array of sentence objects\n",
      "      - sentence_id: unique identifier\n",
      "      - sentence_text: full text (no truncation)\n",
      "      - metadata: company, document, section info\n",
      "\n",
      "✓ Ready for LLM consumption!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Load, Use filtered data and Save Sample to JSON for LLM Processing.\n",
    "\n",
    "import polars as pl\n",
    "import json\n",
    "\n",
    "# Load the filtered data\n",
    "data_path = r\"D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sec_finrag_1M_sample_filtered.parquet\"\n",
    "\n",
    "df = pl.read_parquet(data_path)\n",
    "\n",
    "# Filter for sentences with BOTH flags true\n",
    "high_quality = df.filter(\n",
    "    (pl.col(\"likely_kpi\") == True) & \n",
    "    (pl.col(\"has_numbers\") == True)\n",
    ").head(30)\n",
    "\n",
    "print(\"=\"*120)\n",
    "print(f\"Found {len(high_quality)} high-quality KPI sentences (both likely_kpi=True and has_numbers=True)\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Create structured list for JSON\n",
    "sentences_data = []\n",
    "for row in high_quality.iter_rows(named=True):\n",
    "    sentences_data.append({\n",
    "        \"sentence_id\": row['sentenceID'],\n",
    "        \"sentence_text\": row['sentence'],\n",
    "        \"metadata\": {\n",
    "            \"company_name\": row['name'],\n",
    "            \"cik\": row['cik'],\n",
    "            \"document_id\": row['docID'],\n",
    "            \"section\": row['section'],\n",
    "            \"report_year\": row['report_year'],\n",
    "            \"filing_date\": row['filingDate']\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Create final JSON structure\n",
    "output_json = {\n",
    "    \"dataset\": \"sec_10k_high_quality_kpi_sentences\",\n",
    "    \"filter_criteria\": {\n",
    "        \"likely_kpi\": True,\n",
    "        \"has_numbers\": True\n",
    "    },\n",
    "    \"total_sentences\": len(sentences_data),\n",
    "    \"sentences\": sentences_data\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = r\"D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sample_30_kpi_sentences.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Saved {len(sentences_data)} sentences to JSON\")\n",
    "print(f\"  File: {output_file}\")\n",
    "\n",
    "# Print first 3 as preview\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"PREVIEW - First 3 sentences:\")\n",
    "print(\"=\"*120)\n",
    "print(json.dumps(output_json[\"sentences\"][:3], indent=2, ensure_ascii=False))\n",
    "\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"JSON Structure:\")\n",
    "print(\"=\"*120)\n",
    "print(f\"  - dataset: metadata about the collection\")\n",
    "print(f\"  - filter_criteria: what filters were applied\")\n",
    "print(f\"  - total_sentences: count\")\n",
    "print(f\"  - sentences[]: array of sentence objects\")\n",
    "print(f\"      - sentence_id: unique identifier\")\n",
    "print(f\"      - sentence_text: full text (no truncation)\")\n",
    "print(f\"      - metadata: company, document, section info\")\n",
    "print(f\"\\n✓ Ready for LLM consumption!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61a979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eeac501",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Few errors:\n",
    "- plain string (JSON_GRAMMAR), so the sampler tried to access . _grammar on a string and crashed.\n",
    "- error happens because llama-cpp-python expects a LlamaGrammar object, not a raw string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c57a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... (this will take 10–20s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded in 1.4s\n",
      "\n",
      "Loading filtered dataset...\n",
      "✓ Saved sample of 24 sentences → D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sample_24_kpi_sentences_20251021_180011.json\n",
      "\n",
      "Total sentences: 24 → 2 packs of 12\n",
      "\n",
      "→ Running Pack 1/2 (12 sentences)...\n",
      "→ Running Pack 2/2 (12 sentences)...\n",
      "\n",
      "✓ Extraction completed in 582.9s for 24 sentences (24.29s/sentence avg)\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY: 0 KPIs extracted across 0 sentences\n",
      "====================================================================================================\n",
      "\n",
      "✓ Saved extraction results to D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sample_24_kpi_sentences_20251021_180011_kpi_results.json\n"
     ]
    }
   ],
   "source": [
    "## MAIN ATTEMPT: packing sentences together to attempt bulk querying.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "PACKED KPI Extraction using llama-cpp-python\n",
    "--------------------------------------------\n",
    "- No server, no async.\n",
    "- Two packs of 12 sentences each (from filtered dataset).\n",
    "- Includes grammar and safe delimiters to reduce mixups.\n",
    "- Saves sampled dataset automatically with timestamp.\n",
    "\"\"\"\n",
    "\n",
    "import polars as pl\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp import Llama, LlamaGrammar\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "DATA_PATH = r\"D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sec_finrag_1M_sample_filtered.parquet\"\n",
    "MODEL_PATH = r\"C:\\llama_server\\models\\qwen2p5_7b\\Qwen2.5-7B-Instruct-Q5_K_M.gguf\"\n",
    "\n",
    "N_GPU_LAYERS = -1\n",
    "N_CTX = 4096\n",
    "TEMPERATURE = 0.1\n",
    "MAX_TOKENS = 512  \n",
    "\n",
    "# =============================================================================\n",
    "# LOAD MODEL\n",
    "# =============================================================================\n",
    "print(\"Loading model... (this will take 10–20s)\")\n",
    "t0 = time.time()\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_gpu_layers=N_GPU_LAYERS,\n",
    "    n_ctx=N_CTX,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"✓ Model loaded in {time.time() - t0:.1f}s\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD FILTERED DATA AND SAMPLE 24\n",
    "# =============================================================================\n",
    "print(\"Loading filtered dataset...\")\n",
    "df = pl.read_parquet(DATA_PATH)\n",
    "df_filt = df.filter(\n",
    "    (pl.col(\"likely_kpi\") == True) &\n",
    "    (pl.col(\"has_numbers\") == True)\n",
    ").sample(n=24, seed=int(time.time()))\n",
    "\n",
    "# Save sampled JSON with timestamp\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_json_path = rf\"D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sample_24_kpi_sentences_{now}.json\"\n",
    "\n",
    "sentences = []\n",
    "for row in df_filt.iter_rows(named=True):\n",
    "    sentences.append({\n",
    "        \"sentence_id\": row['sentenceID'],\n",
    "        \"sentence_text\": row['sentence'],\n",
    "        \"metadata\": {\n",
    "            \"company_name\": row['name'],\n",
    "            \"ticker\": row['tickers'],\n",
    "            \"cik\": row['cik'],\n",
    "            \"doc_id\": row['docID'],\n",
    "            \"section\": row['section'],\n",
    "            \"report_year\": row['report_year'],\n",
    "            \"filing_date\": row['filingDate']\n",
    "        }\n",
    "    })\n",
    "\n",
    "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentences, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Saved sample of 24 sentences → {output_json_path}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: DEFINE PACKING LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "PACK_PROMPT = \"\"\"You are a financial KPI extractor.\n",
    "For each input block identified by <<ID>>, extract KPIs into JSON under that same key.\n",
    "Use the EXACT IDs given (do not rename or add).\n",
    "Only use numbers explicitly present in that block.\n",
    "Include both the numeric value and the original substring.\n",
    "\n",
    "Each KPI fields:\n",
    "- category\n",
    "- value\n",
    "- value_raw\n",
    "- unit\n",
    "- year\n",
    "- quarter\n",
    "- period_text\n",
    "- explanation\n",
    "- company\n",
    "- ticker\n",
    "- evidence_sentence_id\n",
    "\n",
    "Input blocks:\n",
    "{pairs}\n",
    "\n",
    "Return only JSON in this form (MINIFIED, one line; no spaces, no newlines):\n",
    "{{\n",
    "  \"<<ID1>>\": [{{\"category\": \"...\", \"value\": 2500, \"unit\": \"USD_millions\", \"company\": \"...\", \"ticker\": \"...\"}}],\n",
    "  \"<<ID2>>\": []\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Simple JSON grammar (helps keep model outputs parseable)\n",
    "JSON_GRAMMAR = r\"\"\"\n",
    "root   ::= object\n",
    "object ::= \"{\" ws (pair (\",\" ws pair)*)? ws \"}\"\n",
    "pair   ::= string ws \":\" ws value\n",
    "array  ::= \"[\" ws (value (\",\" ws value)*)? ws \"]\"\n",
    "value  ::= object | array | string | number | \"true\" | \"false\" | \"null\"\n",
    "string ::= \"\\\"\" ([^\"\\\\] | \"\\\\\" .)* \"\\\"\"\n",
    "number ::= \"-\"? [0-9]+ (\".\" [0-9]+)?\n",
    "ws     ::= [ \\t\\n\\r]*\n",
    "\"\"\"\n",
    "\n",
    "GRAMMAR_OBJ = LlamaGrammar.from_string(JSON_GRAMMAR, \"root\")\n",
    "\n",
    "def build_packs(data: List[Dict], pack_size: int = 12) -> List[List[Dict]]:\n",
    "    \"\"\"Split data into even packs of N\"\"\"\n",
    "    return [data[i:i + pack_size] for i in range(0, len(data), pack_size)]\n",
    "\n",
    "def build_block(pack: List[Dict]) -> str:\n",
    "    \"\"\"Format sentences for input block with IDs and company info\"\"\"\n",
    "    lines = []\n",
    "    for item in pack:\n",
    "        sid = item['sentence_id']\n",
    "        text = item['sentence_text'].replace(\"\\n\", \" \").strip()\n",
    "        meta = item['metadata']\n",
    "        company = meta.get('company_name', 'Unknown')\n",
    "        ticker = meta.get('ticker', '')\n",
    "        lines.append(f\"<<{sid}>> (Company: {company}, Ticker: {ticker}) {text}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "\n",
    "def extract_pack(pack: List[Dict]) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Extract KPIs from a single packed batch.\"\"\"\n",
    "    block = build_block(pack)\n",
    "    prompt = PACK_PROMPT.format(pairs=block)\n",
    "\n",
    "    resp = llm(\n",
    "        prompt,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "        grammar=GRAMMAR_OBJ,   # LlamaGrammar object\n",
    "        # no stop token; avoid truncating valid JSON\n",
    "    )\n",
    "    text = resp[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # 1) Primary parse (fast path)\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Strip code fences if present (```json ... ```)\n",
    "    t = text\n",
    "    if t.startswith(\"```\"):\n",
    "        t = t.strip(\"`\").strip()\n",
    "        if t.lower().startswith(\"json\"):\n",
    "            t = t[4:].lstrip()\n",
    "\n",
    "    # 3) Balanced-brace salvage: take the longest complete {...}\n",
    "    best = \"\"\n",
    "    depth = 0\n",
    "    start = None\n",
    "    for i, ch in enumerate(t):\n",
    "        if ch == \"{\":\n",
    "            if depth == 0:\n",
    "                start = i\n",
    "            depth += 1\n",
    "        elif ch == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0 and start is not None:\n",
    "                cand = t[start:i+1]\n",
    "                if len(cand) > len(best):\n",
    "                    best = cand\n",
    "\n",
    "    if best:\n",
    "        try:\n",
    "            return json.loads(best)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Last-resort regex (may still catch partials)\n",
    "    m = re.search(r\"\\{.*\\}\", t, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 5) Nothing parseable\n",
    "    return {}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: RUN EXTRACTION (2 PACKS x 12)\n",
    "# =============================================================================\n",
    "\n",
    "packs = build_packs(sentences, 12)\n",
    "print(f\"Total sentences: {len(sentences)} → {len(packs)} packs of 12\\n\")\n",
    "\n",
    "results = {}\n",
    "start = time.time()\n",
    "for i, pack in enumerate(packs, 1):\n",
    "    print(f\"→ Running Pack {i}/{len(packs)} ({len(pack)} sentences)...\")\n",
    "    pack_result = extract_pack(pack)\n",
    "    results.update(pack_result)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n✓ Extraction completed in {elapsed:.1f}s for {len(sentences)} sentences \"\n",
    "      f\"({elapsed/len(sentences):.2f}s/sentence avg)\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: SHOW OUTPUT SUMMARY\n",
    "# =============================================================================\n",
    "total_kpis = sum(len(v) for v in results.values())\n",
    "print(\"=\"*100)\n",
    "print(f\"SUMMARY: {total_kpis} KPIs extracted across {len(results)} sentences\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for sid, items in list(results.items())[:3]:\n",
    "    print(f\"\\n--- {sid} ---\")\n",
    "    for item in items:\n",
    "        print({\n",
    "            \"category\": item.get(\"category\"),\n",
    "            \"value\": item.get(\"value\"),\n",
    "            \"unit\": item.get(\"unit\"),\n",
    "            \"company\": item.get(\"company\"),\n",
    "            \"ticker\": item.get(\"ticker\"),\n",
    "            \"year\": item.get(\"year\"),\n",
    "            \"explanation\": item.get(\"explanation\")\n",
    "        })\n",
    "\n",
    "# Optionally save results\n",
    "out_results_path = output_json_path.replace(\".json\", \"_kpi_results.json\")\n",
    "with open(out_results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Saved extraction results to {out_results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47efb6",
   "metadata": {},
   "source": [
    "#### Issue 1 might be that grammar is too restrictive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdc6b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded in 0.9s\n",
      "\n",
      "Loading filtered dataset...\n",
      "✓ Saved sample of 24 sentences → D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sample_8_kpi_sentences_20251022_092310.json\n",
      "\n",
      "Processing 8 sentences individually...\n",
      "====================================================================================================\n",
      "\n",
      "[1/24] ELI LILLY & Co...\n",
      "  Text: In 2017, we recognized $1.33 billion of asset impairment, restructuring, and oth...\n",
      "  • No KPIs extracted\n",
      "\n",
      "[2/24] GENWORTH FINANCIAL INC...\n",
      "  Text: The net proceeds of $397 million from the issuance of the 2024 Notes, together w...\n",
      "  ✓ Found 3 KPIs\n",
      "    • net_proceeds_from_issuance: 397 USD_millions\n",
      "    • contribution_to_gmic: 100 USD_millions\n",
      "    • contribution_to_us_mortgage_holding_company: 300 USD_millions\n",
      "\n",
      "[3/24] GENWORTH FINANCIAL INC...\n",
      "  Text: The following table sets forth the increase (decrease) in amortization of DAC re...\n",
      "  • No KPIs extracted\n",
      "\n",
      "[4/24] WASHINGTON TRUST BANCORP INC...\n",
      "  Text: Approximately 80% of the net client outflows were associated with the loss of ce...\n",
      "  • No KPIs extracted\n",
      "\n",
      "[5/24] ABBOTT LABORATORIES...\n",
      "  Text: The estimated annual amortization expense for intangible assets recorded at Dece...\n",
      "  • No KPIs extracted\n",
      "\n",
      "[6/24] AMERICAN STATES WATER CO...\n",
      "  Text: GSWC’s maximum ability to pay dividends is restricted by certain Note Agreements...\n",
      "  • No KPIs extracted\n",
      "\n",
      "[7/24] THERMO FISHER SCIENTIFIC INC....\n",
      "  Text: (c) Excludes non-cash charges, net, of $1.9 million and a loss of $1.7 million f...\n",
      "  • No KPIs extracted\n",
      "\n",
      "[8/24] BGC Partners, Inc....\n",
      "  Text: On February 9, 2016, the Audit Committee of the Board of Directors authorized th...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 217\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/24] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany[:\u001b[32m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msent[\u001b[33m'\u001b[39m\u001b[33msentence_text\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m80\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m kpis = \u001b[43mextract_single_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m all_results[sid] = kpis\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kpis:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 159\u001b[39m, in \u001b[36mextract_single_sentence\u001b[39m\u001b[34m(sentence_data)\u001b[39m\n\u001b[32m     92\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mExtract ALL financial and operational metrics from this SEC 10-K sentence.\u001b[39m\n\u001b[32m     93\u001b[39m \n\u001b[32m     94\u001b[39m \u001b[33mCompany: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta[\u001b[33m'\u001b[39m\u001b[33mcompany_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m \n\u001b[32m    155\u001b[39m \u001b[33mJSON array:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         resp = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased for richer output\u001b[39;49;00m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Deterministic\u001b[39;49;00m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m```\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mInput:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mNOW EXTRACT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m         output = resp[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].strip()\n\u001b[32m    169\u001b[39m         \u001b[38;5;66;03m# Clean JSON extraction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joems\\miniconda3\\envs\\finrag_mlops\\Lib\\site-packages\\llama_cpp\\llama.py:1904\u001b[39m, in \u001b[36mLlama.__call__\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1840\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1841\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1842\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1866\u001b[39m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1867\u001b[39m ) -> Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[32m   1868\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[32m   1869\u001b[39m \n\u001b[32m   1870\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1902\u001b[39m \u001b[33;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[32m   1903\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m=\u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joems\\miniconda3\\envs\\finrag_mlops\\Lib\\site-packages\\llama_cpp\\llama.py:1837\u001b[39m, in \u001b[36mLlama.create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1835\u001b[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001b[32m   1836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[32m-> \u001b[39m\u001b[32m1837\u001b[39m completion: Completion = \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joems\\miniconda3\\envs\\finrag_mlops\\Lib\\site-packages\\llama_cpp\\llama.py:1322\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1320\u001b[39m finish_reason = \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1321\u001b[39m multibyte_fix = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joems\\miniconda3\\envs\\finrag_mlops\\Lib\\site-packages\\llama_cpp\\llama.py:914\u001b[39m, in \u001b[36mLlama.generate\u001b[39m\u001b[34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[39m\n\u001b[32m    912\u001b[39m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    915\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx < \u001b[38;5;28mself\u001b[39m.n_tokens:\n\u001b[32m    916\u001b[39m         token = \u001b[38;5;28mself\u001b[39m.sample(\n\u001b[32m    917\u001b[39m             top_k=top_k,\n\u001b[32m    918\u001b[39m             top_p=top_p,\n\u001b[32m   (...)\u001b[39m\u001b[32m    932\u001b[39m             idx=sample_idx,\n\u001b[32m    933\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joems\\miniconda3\\envs\\finrag_mlops\\Lib\\site-packages\\llama_cpp\\llama.py:648\u001b[39m, in \u001b[36mLlama.eval\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    644\u001b[39m n_tokens = \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    645\u001b[39m \u001b[38;5;28mself\u001b[39m._batch.set_batch(\n\u001b[32m    646\u001b[39m     batch=batch, n_past=n_past, logits_all=\u001b[38;5;28mself\u001b[39m._logits_all\n\u001b[32m    647\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[32m    650\u001b[39m \u001b[38;5;28mself\u001b[39m.input_ids[n_past : n_past + n_tokens] = batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joems\\miniconda3\\envs\\finrag_mlops\\Lib\\site-packages\\llama_cpp\\_internals.py:322\u001b[39m, in \u001b[36mLlamaContext.decode\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     return_code = \u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_code != \u001b[32m0\u001b[39m:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FIXED Packed KPI Extraction using llama-cpp-python\n",
    "--------------------------------------------------\n",
    "- Processes sentences in packs of 6 (not 12 - more reliable)\n",
    "- Simpler prompt structure that works with the model\n",
    "- No grammar (it was causing the issue)\n",
    "- Better JSON parsing with fallbacks\n",
    "\"\"\"\n",
    "\n",
    "import polars as pl\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "DATA_PATH = r\"D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sec_finrag_1M_sample_filtered.parquet\"\n",
    "MODEL_PATH = r\"C:\\llama_server\\models\\qwen2p5_7b\\Qwen2.5-7B-Instruct-Q5_K_M.gguf\"\n",
    "\n",
    "N_GPU_LAYERS = -1\n",
    "N_CTX = 8192  # Increased for longer prompts\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 1024  # Increased for multiple sentence responses\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD MODEL\n",
    "# =============================================================================\n",
    "print(\"Loading model...\")\n",
    "t0 = time.time()\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_gpu_layers=N_GPU_LAYERS,\n",
    "    n_ctx=N_CTX,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded in {time.time() - t0:.1f}s\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD FILTERED DATA AND SAMPLE N\n",
    "# =============================================================================\n",
    "print(\"Loading filtered dataset...\")\n",
    "df = pl.read_parquet(DATA_PATH)\n",
    "\n",
    "num = 8  # sentences to sample\n",
    "df_filt = df.filter(\n",
    "    (pl.col(\"likely_kpi\") == True) &\n",
    "    (pl.col(\"has_numbers\") == True)\n",
    ").sample(n=num, seed=42)  # Fixed seed for reproducibility\n",
    "\n",
    "# Save sampled JSON with timestamp\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_json_path = rf\"D:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\finrag-insights-mlops\\data\\exports\\sample_{num}_kpi_sentences_{now}.json\"\n",
    "\n",
    "sentences = []\n",
    "for row in df_filt.iter_rows(named=True):\n",
    "    sentences.append({\n",
    "        \"sentence_id\": row['sentenceID'],\n",
    "        \"sentence_text\": row['sentence'],\n",
    "        \"metadata\": {\n",
    "            \"company_name\": row['name'],\n",
    "            \"ticker\": row['tickers'],\n",
    "            \"cik\": row['cik'],\n",
    "            \"doc_id\": row['docID'],\n",
    "            \"section\": row['section'],\n",
    "            \"report_year\": row['report_year'],\n",
    "            \"filing_date\": row['filingDate']\n",
    "        }\n",
    "    })\n",
    "\n",
    "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentences, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Saved sample of {num} sentences → {output_json_path}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: SIMPLER PACK EXTRACTION (One sentence at a time in context)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_single_sentence(sentence_data: Dict) -> List[Dict]:\n",
    "    \"\"\"Extract KPIs with RICH schema and dynamic categorization\"\"\"\n",
    "    \n",
    "    sid = sentence_data['sentence_id']\n",
    "    text = sentence_data['sentence_text']\n",
    "    meta = sentence_data['metadata']\n",
    "    \n",
    "    # IMPROVED: Comprehensive prompt with flexible schema\n",
    "    prompt = f\"\"\"Extract ALL financial and operational metrics from this SEC 10-K sentence.\n",
    "\n",
    "Company: {meta['company_name']}\n",
    "Ticker: {meta['ticker']}\n",
    "Sentence: {text}\n",
    "\n",
    "YOU ARE A FINANCIAL ANALYST. Extract both monetary AND non-monetary metrics.\n",
    "\n",
    "For EACH metric found, create a JSON object with these fields:\n",
    "\n",
    "REQUIRED FIELDS:\n",
    "- \"category\": Use standard terms if obvious (revenue, debt, employees, etc.). If not obvious, study and infer a descriptive 2-4 word category that captures the business meaning (e.g., \"valuation_allowance_release\", \"inter_segment_sales\", \"restructuring_charges\", \"basis_point_change\")\n",
    "- \"value\": The numeric value (convert to consistent units - see below)\n",
    "- \"value_raw\": The EXACT text from sentence showing this number (e.g., \"$392 million\", \"11 basis points\", \"1,810 employees\")\n",
    "- \"unit\": Choose from: percent, basis_points, USD_millions, USD_billions, USD_thousands, count, ratio, BOE (barrels oil equivalent), or create appropriate unit\n",
    "\n",
    "OPTIONAL FIELDS (include if present):\n",
    "- \"year\": Fiscal year if mentioned (2017, 2018, etc.)\n",
    "- \"quarter\": If mentioned (Q1, Q2, Q3, Q4)\n",
    "- \"period_text\": Original period description (\"fiscal year ended December 31, 2019\")\n",
    "- \"comparison_year\": If comparing to prior period (e.g., \"compared to 2016\")\n",
    "- \"metric_type\": \"absolute\" or \"change\" or \"rate\" or \"percentage_change\"\n",
    "- \"context\": Brief explanation of what this metric represents (max 10 words)\n",
    "\n",
    "UNIT CONVERSION RULES:\n",
    "- \"$25 million\" → {{\"value\": 25, \"unit\": \"USD_millions\"}}\n",
    "- \"$41,057\" or \"$41 thousand\" → {{\"value\": 0.041, \"unit\": \"USD_millions\"}} \n",
    "- \"11 basis points\" → {{\"value\": 11, \"unit\": \"basis_points\"}}\n",
    "- \"1,810 employees\" → {{\"value\": 1810, \"unit\": \"count\"}}\n",
    "- \"2.63%\" → {{\"value\": 2.63, \"unit\": \"percent\"}}\n",
    "\n",
    "EXAMPLES OF GOOD EXTRACTION:\n",
    "\n",
    "Input: \"Does not include inter-segment sales of $392 million and $405 million in 2017 and 2016.\"\n",
    "Output: [\n",
    "  {{\"category\": \"inter_segment_sales_excluded\", \"value\": 392, \"value_raw\": \"$392 million\", \"unit\": \"USD_millions\", \"year\": 2017, \"context\": \"excluded from segment totals\"}},\n",
    "  {{\"category\": \"inter_segment_sales_excluded\", \"value\": 405, \"value_raw\": \"$405 million\", \"unit\": \"USD_millions\", \"year\": 2016, \"context\": \"excluded from segment totals\"}}\n",
    "]\n",
    "\n",
    "Input: \"The $25 million deposit is included in Cash segregated under regulatory requirements.\"\n",
    "Output: [\n",
    "  {{\"category\": \"regulatory_cash_deposit\", \"value\": 25, \"value_raw\": \"$25 million\", \"unit\": \"USD_millions\", \"context\": \"segregated cash requirement\"}}\n",
    "]\n",
    "\n",
    "Input: \"The net gain of $41,057 is included in Other Income.\"\n",
    "Output: [\n",
    "  {{\"category\": \"other_income_gain\", \"value\": 0.041, \"value_raw\": \"$41,057\", \"unit\": \"USD_millions\", \"context\": \"net gain in other income\"}}\n",
    "]\n",
    "\n",
    "Input: \"FTE rate of return on securities was 2.63% in 2018, up by 11 basis points.\"\n",
    "Output: [\n",
    "  {{\"category\": \"return_on_securities_fte\", \"value\": 2.63, \"value_raw\": \"2.63%\", \"unit\": \"percent\", \"year\": 2018, \"metric_type\": \"rate\"}},\n",
    "  {{\"category\": \"return_on_securities_change\", \"value\": 11, \"value_raw\": \"11 basis points\", \"unit\": \"basis_points\", \"metric_type\": \"change\", \"context\": \"year over year increase\"}}\n",
    "]\n",
    "\n",
    "Input: \"Employee separation charges relate to severance for approximately 1,810 and 2,720 employees in 2019 and 2018.\"\n",
    "Output: [\n",
    "  {{\"category\": \"employee_separations\", \"value\": 1810, \"value_raw\": \"1,810 employees\", \"unit\": \"count\", \"year\": 2019, \"context\": \"restructuring headcount\"}},\n",
    "  {{\"category\": \"employee_separations\", \"value\": 2720, \"value_raw\": \"2,720 employees\", \"unit\": \"count\", \"year\": 2018, \"context\": \"restructuring headcount\"}}\n",
    "]\n",
    "\n",
    "NOW EXTRACT FROM THIS SENTENCE. Be comprehensive - capture EVERY number with business meaning.\n",
    "\n",
    "JSON array:\"\"\"\n",
    "    \n",
    "\n",
    "    try:\n",
    "        resp = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,  # Increased for richer output\n",
    "            temperature=0.0,  # Deterministic\n",
    "            stop=[\"```\", \"\\n\\nInput:\", \"\\n\\nNOW EXTRACT\"],\n",
    "            repeat_penalty=1.1,\n",
    "        )\n",
    "        \n",
    "        output = resp[\"choices\"][0][\"text\"].strip()\n",
    "        \n",
    "        # Clean JSON extraction\n",
    "        output = re.sub(r'```json\\s*|\\s*```', '', output)\n",
    "        \n",
    "        # Find JSON array\n",
    "        array_match = re.search(r'\\[.*?\\]', output, re.DOTALL)\n",
    "        if array_match:\n",
    "            json_str = array_match.group(0)\n",
    "            \n",
    "            # Fix common JSON issues\n",
    "            json_str = json_str.replace(\"'\", '\"')  # Single to double quotes\n",
    "            json_str = re.sub(r',(\\s*[}\\]])', r'\\1', json_str)  # Trailing commas\n",
    "            \n",
    "            kpis = json.loads(json_str)\n",
    "            \n",
    "            # Add sentence-level metadata to each KPI\n",
    "            for kpi in kpis:\n",
    "                kpi['sentence_id'] = sid\n",
    "                kpi['company'] = meta['company_name']\n",
    "                kpi['ticker'] = meta['ticker']\n",
    "                kpi['source_sentence'] = text  # Keep full sentence for context\n",
    "            \n",
    "            return kpis if isinstance(kpis, list) else []\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"  ⚠ JSON parse error: {str(e)[:50]}\")\n",
    "        print(f\"  Raw output: {output[:200]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Error on {sid}: {str(e)[:50]}\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: PROCESS ALL SENTENCES\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Processing {len(sentences)} sentences individually...\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "all_results = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    sid = sent['sentence_id']\n",
    "    company = sent['metadata']['company_name']\n",
    "    \n",
    "    print(f\"\\n[{i}/24] {company[:40]}...\")\n",
    "    print(f\"  Text: {sent['sentence_text'][:80]}...\")\n",
    "    \n",
    "    kpis = extract_single_sentence(sent)\n",
    "    all_results[sid] = kpis\n",
    "    \n",
    "    if kpis:\n",
    "        print(f\"  ✓ Found {len(kpis)} KPIs\")\n",
    "        for kpi in kpis:\n",
    "            print(f\"    • {kpi.get('category')}: {kpi.get('value')} {kpi.get('unit')}\")\n",
    "    else:\n",
    "        print(f\"  • No KPIs extracted\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "avg_time = elapsed / len(sentences)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: SUMMARY & SAVE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EXTRACTION COMPLETE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "total_kpis = sum(len(v) for v in all_results.values())\n",
    "sentences_with_kpis = sum(1 for v in all_results.values() if v)\n",
    "\n",
    "print(f\"Total sentences:       {len(sentences)}\")\n",
    "print(f\"Sentences with KPIs:   {sentences_with_kpis}/{len(sentences)} ({sentences_with_kpis/len(sentences)*100:.1f}%)\")\n",
    "print(f\"Total KPIs extracted:  {total_kpis}\")\n",
    "print(f\"Average per sentence:  {total_kpis/len(sentences):.2f}\")\n",
    "print(f\"\\nProcessing time:       {elapsed:.1f}s\")\n",
    "print(f\"Average per sentence:  {avg_time:.2f}s\")\n",
    "print(f\"Throughput:            {60/avg_time:.1f} sentences/minute\")\n",
    "\n",
    "# Extrapolate\n",
    "print(f\"\\nExtrapolation for 564,551 filtered sentences:\")\n",
    "print(f\"  Sequential:          {564551 * avg_time / 3600:.1f} hours\")\n",
    "print(f\"  With 10 parallel:    {564551 * avg_time / 3600 / 10:.1f} hours\")\n",
    "\n",
    "# Save results\n",
    "out_results_path = output_json_path.replace(\".json\", \"_kpi_results.json\")\n",
    "with open(out_results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {out_results_path}\")\n",
    "print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293a88b",
   "metadata": {},
   "source": [
    "#### Issue 2:\n",
    "- Quantization drift / Context drift. Model precision degrades over time.\n",
    "- No few-shot guidance.\n",
    "- accumulation (even though we're not explicitly passing history, the model's internal state affects subsequent generations)\n",
    "- successful patterns can create bias for later failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53eeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048244c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9444978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84297bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876a02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finrag_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
