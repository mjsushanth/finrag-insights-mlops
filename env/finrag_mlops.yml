name: finrag_mlops
channels:
  - pytorch
  - nvidia
  - conda-forge
  - defaults
dependencies:
  # ---- Core runtime ----
  - python=3.11
  - pip
  - numpy<2.0
  - pandas
  - polars>=0.20.0
  - pyarrow>=14.0.0
  - scipy
  - scikit-learn
  - tqdm
  - rich

  # ---- PyTorch GPU Stack (Windows-compatible) ----
  - pytorch>=2.1.0
  - pytorch-cuda=11.8
  - torchvision
  - torchaudio

  # ---- Hugging Face Ecosystem ----
  - datasets
  - huggingface_hub
  - tokenizers>=0.15.0
  - transformers>=4.35.0
  - accelerate>=0.24.0
  - sentencepiece
  - safetensors

  # ---- Data Processing ----
  - duckdb>=0.9.0
  - fastparquet
  
  # ---- Vector search ----
  - faiss-cpu

  # ---- NLP Tools ----
  - spacy>=3.7.0
  - nltk>=3.8.0

  # ---- API & config ----
  - fastapi>=0.104.0
  - uvicorn
  - pydantic>=2.0
  - python-dotenv
  - pyyaml

  # ---- Notebooks ----
  - ipykernel
  - jupyterlab>=4.0
  - matplotlib
  - seaborn
  - plotly>=5.18.0
  - ipywidgets>=8.1.0

  # ---- Quality ----
  - pytest
  - black
  - ruff>=0.1.0

  # ---- Performance Monitoring ----
  - psutil
  
  # ---- Build tools for llama-cpp-python ----
  - cmake
  - ninja
  
  - cryptography>=42.0    # Fernet encryption/decryption
  - keyring>=24.0         # OS keychain (Win Credential Manager / macOS Keychain / GNOME)

  - pip:
      # ---- Core ML/NLP ----
      - sentence-transformers>=2.7.0
      - qdrant-client>=1.9.0
      
      # ---- GPU Optimization ----
      - bitsandbytes-windows
      - pynvml
      
      # ---- Model Optimization ----
      - optimum>=1.15.0
      
      # ---- Text Processing ----
      - ftfy>=6.1.0
      - unidecode>=1.3.0
      
      # ---- Monitoring ----
      - wandb>=0.16.0
      
      # ---- Transformer dependencies ----
      - protobuf>=3.20.0
      - sacremoses

      # ---- Display and formatting tools ----
      - itables>=1.7.0
      - tabulate>=0.9.0
      - IPython>=8.0.0
      - jupyter>=1.0.0

      - openai

      # ---- CRITICAL: llama-cpp-python with CUDA support ----

      # This will be installed with CMAKE_ARGS after environment creation

      # See instructions below -----------------------------------------
      # # Remove any existing installation
      # pip uninstall llama-cpp-python -y

      # # Set CUDA flag and install
      # set CMAKE_ARGS=-DGGML_CUDA=on
      # pip install llama-cpp-python --no-cache-dir --force-reinstall --upgrade